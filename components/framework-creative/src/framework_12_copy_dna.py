#!/usr/bin/env python3
"""
Framework 12: Copy DNA Audit (ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°èªå½™ãƒ»ãƒªã‚ºãƒ ãƒ»æ§‹æ–‡åˆ†æ)
Creative Frameworks Component

å®Ÿè¡Œæ–¹æ³•: 
  python src/framework_12_copy_dna.py
  
ç‹¬ç«‹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¨ã—ã¦å‹•ä½œã—ã¾ã™ã€‚
"""

import asyncio
import json
import time
import argparse
from datetime import datetime
from typing import List, Dict, Any, Tuple
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import sys
import re
from collections import Counter, defaultdict
import numpy as np

# ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒã‚§ãƒƒã‚¯
try:
    import jieba
    import wordcloud
except ImportError:
    print("âŒ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
    print("pip install jieba wordcloud matplotlib seaborn pandas numpy")
    sys.exit(1)

class CopyDNAFramework:
    """Copy DNA Audit ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ - ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ç‰ˆ"""
    
    def __init__(self, output_base_dir: str = "output"):
        self.framework_id = 12
        self.framework_name = "Copy DNA Audit"
        self.version = "1.0.0"
        self.component_name = "framework-creative"
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå†…ã«è¨­å®š
        self.output_dir = os.path.join(
            output_base_dir, 
            f"framework_12_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(self.output_dir, exist_ok=True)
        
        print(f"ğŸš€ {self.framework_name} v{self.version}")
        print(f"ğŸ“¦ Component: {self.component_name}")
        print(f"ğŸ“ Output: {self.output_dir}")
        
        # æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆåˆ†æç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.hiragana_pattern = re.compile(r'[ã²ã‚‰ãŒãª]+')
        self.katakana_pattern = re.compile(r'[ã‚«ã‚¿ã‚«ãƒŠ]+')
        self.kanji_pattern = re.compile(r'[ä¸€-é¾¯]+')
        self.punctuation_pattern = re.compile(r'[ã€‚ã€ï¼ï¼Ÿâ€¦ãƒ»â€»]')
        
    async def collect_data(self, copy_samples: List[str], brand_name: str = "Unknown") -> Dict[str, Any]:
        """Copy DNA ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»åˆ†æ"""
        
        start_time = time.time()
        print(f"\nğŸ“Š åˆ†æé–‹å§‹: {len(copy_samples)}ä»¶ã®ã‚³ãƒ”ãƒ¼åˆ†æ")
        
        try:
            # Step 1: åŸºæœ¬çµ±è¨ˆåˆ†æ
            print("ğŸ“ åŸºæœ¬çµ±è¨ˆåˆ†æä¸­...")
            basic_stats = self._analyze_basic_statistics(copy_samples)
            
            # Step 2: èªå½™åˆ†æ
            print("ğŸ”¤ èªå½™åˆ†æä¸­...")
            vocabulary_analysis = await self._analyze_vocabulary(copy_samples)
            
            # Step 3: ãƒªã‚ºãƒ ãƒ»éŸ³éŸ»åˆ†æ
            print("ğŸµ ãƒªã‚ºãƒ åˆ†æä¸­...")
            rhythm_analysis = self._analyze_rhythm_patterns(copy_samples)
            
            # Step 4: æ§‹æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            print("ğŸ—ï¸ æ§‹æ–‡åˆ†æä¸­...")
            syntax_analysis = self._analyze_syntax_patterns(copy_samples)
            
            # Step 5: æ„Ÿæƒ…ãƒ»ãƒˆãƒ¼ãƒ³åˆ†æ
            print("ğŸ˜Š æ„Ÿæƒ…ãƒˆãƒ¼ãƒ³åˆ†æä¸­...")
            emotion_analysis = self._analyze_emotional_tone(copy_samples)
            
            # Step 6: DNAãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º
            print("ğŸ§¬ DNAãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡ºä¸­...")
            dna_patterns = self._extract_dna_patterns(
                vocabulary_analysis, rhythm_analysis, syntax_analysis, emotion_analysis
            )
            
            execution_time = time.time() - start_time
            
            result = {
                "framework_info": {
                    "id": self.framework_id,
                    "name": self.framework_name,
                    "version": self.version,
                    "component": self.component_name,
                    "execution_time": execution_time,
                    "timestamp": datetime.now().isoformat()
                },
                "input_parameters": {
                    "brand_name": brand_name,
                    "copy_count": len(copy_samples),
                    "sample_copies": copy_samples[:3]  # æœ€åˆã®3ä»¶ã‚’ã‚µãƒ³ãƒ—ãƒ«ã¨ã—ã¦ä¿å­˜
                },
                "basic_statistics": basic_stats,
                "vocabulary_analysis": vocabulary_analysis,
                "rhythm_analysis": rhythm_analysis,
                "syntax_analysis": syntax_analysis,
                "emotion_analysis": emotion_analysis,
                "dna_patterns": dna_patterns,
                "insights": self._generate_insights(basic_stats, vocabulary_analysis, dna_patterns),
                "success": True
            }
            
            print(f"âœ… åˆ†æå®Œäº† ({execution_time:.2f}ç§’)")
            return result
            
        except Exception as e:
            error_result = {
                "framework_info": {
                    "id": self.framework_id,
                    "name": self.framework_name,
                    "component": self.component_name,
                    "error": str(e)
                },
                "success": False,
                "execution_time": time.time() - start_time
            }
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return error_result
    
    def _analyze_basic_statistics(self, copies: List[str]) -> Dict[str, Any]:
        """åŸºæœ¬çµ±è¨ˆåˆ†æ"""
        char_counts = [len(copy) for copy in copies]
        word_counts = [len(copy.split()) for copy in copies]
        sentence_counts = [len(re.split(r'[ã€‚ï¼ï¼Ÿ]', copy)) for copy in copies]
        
        return {
            "total_copies": len(copies),
            "character_stats": {
                "avg": np.mean(char_counts),
                "median": np.median(char_counts),
                "min": min(char_counts),
                "max": max(char_counts),
                "std": np.std(char_counts)
            },
            "word_stats": {
                "avg": np.mean(word_counts),
                "median": np.median(word_counts),
                "std": np.std(word_counts)
            },
            "sentence_stats": {
                "avg": np.mean(sentence_counts),
                "median": np.median(sentence_counts),
                "std": np.std(sentence_counts)
            }
        }
    
    async def _analyze_vocabulary(self, copies: List[str]) -> Dict[str, Any]:
        """èªå½™åˆ†æ"""
        all_text = ' '.join(copies)
        
        # å½¢æ…‹ç´ è§£æï¼ˆç°¡æ˜“ç‰ˆï¼‰
        words = []
        for copy in copies:
            # ã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ã®åˆ†é›¢
            hiragana_words = re.findall(r'[ã²ã‚‰ãŒãª]+', copy)
            katakana_words = re.findall(r'[ã‚¡-ãƒ¶]+', copy)
            kanji_words = re.findall(r'[ä¸€-é¾¯]+', copy)
            
            words.extend(hiragana_words + katakana_words + kanji_words)
        
        # èªå½™çµ±è¨ˆ
        word_freq = Counter(words)
        unique_words = len(word_freq)
        total_words = sum(word_freq.values())
        
        # N-gramåˆ†æ
        bigrams = self._extract_ngrams(all_text, 2)
        trigrams = self._extract_ngrams(all_text, 3)
        
        # æ–‡å­—ç¨®åˆ¥åˆ†æ
        char_type_ratio = self._analyze_character_types(all_text)
        
        return {
            "vocabulary_size": unique_words,
            "total_words": total_words,
            "vocabulary_richness": unique_words / total_words if total_words > 0 else 0,
            "top_words": dict(word_freq.most_common(20)),
            "top_bigrams": dict(Counter(bigrams).most_common(10)),
            "top_trigrams": dict(Counter(trigrams).most_common(10)),
            "character_type_ratio": char_type_ratio,
            "rare_words": [word for word, freq in word_freq.items() if freq == 1][:10]
        }
    
    def _extract_ngrams(self, text: str, n: int) -> List[str]:
        """N-gramæŠ½å‡º"""
        # ç°¡æ˜“ç‰ˆï¼šæ–‡å­—å˜ä½ã®N-gram
        cleaned_text = re.sub(r'[^\w]', '', text)
        return [cleaned_text[i:i+n] for i in range(len(cleaned_text)-n+1)]
    
    def _analyze_character_types(self, text: str) -> Dict[str, float]:
        """æ–‡å­—ç¨®åˆ¥æ¯”ç‡åˆ†æ"""
        total_chars = len(re.sub(r'\s', '', text))
        
        hiragana_count = len(re.findall(r'[ã²ã‚‰ãŒãª]', text))
        katakana_count = len(re.findall(r'[ã‚¡-ãƒ¶]', text))
        kanji_count = len(re.findall(r'[ä¸€-é¾¯]', text))
        
        if total_chars == 0:
            return {"hiragana": 0, "katakana": 0, "kanji": 0, "other": 0}
        
        return {
            "hiragana": hiragana_count / total_chars,
            "katakana": katakana_count / total_chars,
            "kanji": kanji_count / total_chars,
            "other": (total_chars - hiragana_count - katakana_count - kanji_count) / total_chars
        }
    
    def _analyze_rhythm_patterns(self, copies: List[str]) -> Dict[str, Any]:
        """ãƒªã‚ºãƒ ãƒ»éŸ³éŸ»åˆ†æ"""
        
        # éŸ³ç¯€æ•°åˆ†æ
        syllable_counts = []
        punctuation_frequencies = []
        
        for copy in copies:
            # ç°¡æ˜“éŸ³ç¯€ã‚«ã‚¦ãƒ³ãƒˆï¼ˆã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠ1æ–‡å­— = 1éŸ³ç¯€ï¼‰
            syllables = len(re.findall(r'[ã²ã‚‰ãŒãªã‚¡-ãƒ¶]', copy))
            syllable_counts.append(syllables)
            
            # å¥èª­ç‚¹é »åº¦
            punct_count = len(re.findall(r'[ã€‚ã€ï¼ï¼Ÿâ€¦ãƒ»â€»]', copy))
            punctuation_frequencies.append(punct_count)
        
        # ãƒªã‚ºãƒ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†é¡
        rhythm_types = self._classify_rhythm_types(copies)
        
        # éŸ³éŸ»ç‰¹å¾´
        alliteration_score = self._calculate_alliteration_score(copies)
        
        return {
            "syllable_stats": {
                "avg": np.mean(syllable_counts) if syllable_counts else 0,
                "median": np.median(syllable_counts) if syllable_counts else 0,
                "std": np.std(syllable_counts) if syllable_counts else 0
            },
            "punctuation_frequency": {
                "avg": np.mean(punctuation_frequencies) if punctuation_frequencies else 0,
                "total": sum(punctuation_frequencies)
            },
            "rhythm_types": rhythm_types,
            "alliteration_score": alliteration_score,
            "tempo_classification": self._classify_tempo(syllable_counts, punctuation_frequencies)
        }
    
    def _classify_rhythm_types(self, copies: List[str]) -> Dict[str, int]:
        """ãƒªã‚ºãƒ ã‚¿ã‚¤ãƒ—åˆ†é¡"""
        rhythm_counts = {"short_burst": 0, "medium_flow": 0, "long_narrative": 0}
        
        for copy in copies:
            char_count = len(copy)
            if char_count <= 20:
                rhythm_counts["short_burst"] += 1
            elif char_count <= 50:
                rhythm_counts["medium_flow"] += 1
            else:
                rhythm_counts["long_narrative"] += 1
        
        return rhythm_counts
    
    def _calculate_alliteration_score(self, copies: List[str]) -> float:
        """é ­éŸ»ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        # ç°¡æ˜“ç‰ˆï¼šéš£æ¥ã™ã‚‹å˜èªã®æœ€åˆã®æ–‡å­—ãŒåŒã˜å ´åˆã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        total_score = 0
        total_pairs = 0
        
        for copy in copies:
            words = re.findall(r'[ã²ã‚‰ãŒãªã‚¡-ãƒ¶ä¸€-é¾¯]+', copy)
            for i in range(len(words) - 1):
                if len(words[i]) > 0 and len(words[i + 1]) > 0:
                    if words[i][0] == words[i + 1][0]:
                        total_score += 1
                    total_pairs += 1
        
        return total_score / total_pairs if total_pairs > 0 else 0
    
    def _classify_tempo(self, syllable_counts: List[int], punct_counts: List[int]) -> str:
        """ãƒ†ãƒ³ãƒåˆ†é¡"""
        if not syllable_counts:
            return "unknown"
        
        avg_syllables = np.mean(syllable_counts)
        avg_punct = np.mean(punct_counts)
        
        if avg_syllables < 10 and avg_punct > 1:
            return "staccato"  # çŸ­ãåˆ‡ã‚Œå‘³ã®ã‚ã‚‹ãƒ†ãƒ³ãƒ
        elif avg_syllables > 30 and avg_punct < 2:
            return "legato"    # æµã‚Œã‚‹ã‚ˆã†ãªãƒ†ãƒ³ãƒ
        else:
            return "moderato"  # ä¸­åº¸ãªãƒ†ãƒ³ãƒ
    
    def _analyze_syntax_patterns(self, copies: List[str]) -> Dict[str, Any]:
        """æ§‹æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        
        # æ–‡ã®é•·ã•åˆ†æ
        sentence_lengths = []
        question_count = 0
        exclamation_count = 0
        
        for copy in copies:
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', copy)
            sentences = [s.strip() for s in sentences if s.strip()]
            
            for sentence in sentences:
                sentence_lengths.append(len(sentence))
            
            question_count += copy.count('ï¼Ÿ')
            exclamation_count += copy.count('ï¼')
        
        # ä¿®è¾æŠ€æ³•æ¤œå‡º
        rhetorical_devices = self._detect_rhetorical_devices(copies)
        
        # è¤‡æ–‡æ¯”ç‡
        complex_sentence_ratio = self._calculate_complex_sentence_ratio(copies)
        
        return {
            "sentence_length_stats": {
                "avg": np.mean(sentence_lengths) if sentence_lengths else 0,
                "median": np.median(sentence_lengths) if sentence_lengths else 0,
                "std": np.std(sentence_lengths) if sentence_lengths else 0
            },
            "question_ratio": question_count / len(copies) if copies else 0,
            "exclamation_ratio": exclamation_count / len(copies) if copies else 0,
            "rhetorical_devices": rhetorical_devices,
            "complex_sentence_ratio": complex_sentence_ratio,
            "syntax_variety_score": len(set(sentence_lengths)) / len(sentence_lengths) if sentence_lengths else 0
        }
    
    def _detect_rhetorical_devices(self, copies: List[str]) -> Dict[str, int]:
        """ä¿®è¾æŠ€æ³•æ¤œå‡º"""
        devices = {
            "repetition": 0,      # åå¾©
            "parallelism": 0,     # ä¸¦åˆ—
            "contrast": 0,        # å¯¾æ¯”
            "metaphor": 0         # æ¯”å–©
        }
        
        for copy in copies:
            # ç°¡æ˜“æ¤œå‡º
            words = re.findall(r'[ã²ã‚‰ãŒãªã‚¡-ãƒ¶ä¸€-é¾¯]+', copy)
            word_freq = Counter(words)
            
            # åå¾©æ¤œå‡ºï¼ˆåŒã˜èªãŒ2å›ä»¥ä¸Šå‡ºç¾ï¼‰
            repeated_words = [word for word, freq in word_freq.items() if freq >= 2]
            devices["repetition"] += len(repeated_words)
            
            # å¯¾æ¯”è¡¨ç¾æ¤œå‡º
            contrast_patterns = ['ã—ã‹ã—', 'ã§ã‚‚', 'ã ãŒ', 'ã¨ã“ã‚ãŒ', 'ã‘ã‚Œã©']
            for pattern in contrast_patterns:
                if pattern in copy:
                    devices["contrast"] += 1
                    break
        
        return devices
    
    def _calculate_complex_sentence_ratio(self, copies: List[str]) -> float:
        """è¤‡æ–‡æ¯”ç‡è¨ˆç®—"""
        total_sentences = 0
        complex_sentences = 0
        
        complex_indicators = ['ãŒ', 'ã®ã§', 'ã‹ã‚‰', 'ã‘ã‚Œã©', 'ã‚‚ã®ã®', 'ãŸã‚']
        
        for copy in copies:
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', copy)
            sentences = [s.strip() for s in sentences if s.strip()]
            
            for sentence in sentences:
                total_sentences += 1
                for indicator in complex_indicators:
                    if indicator in sentence:
                        complex_sentences += 1
                        break
        
        return complex_sentences / total_sentences if total_sentences > 0 else 0
    
    def _analyze_emotional_tone(self, copies: List[str]) -> Dict[str, Any]:
        """æ„Ÿæƒ…ãƒ»ãƒˆãƒ¼ãƒ³åˆ†æ"""
        
        # æ„Ÿæƒ…èªè¾æ›¸ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        emotion_words = {
            "positive": ["å¬‰ã—ã„", "æ¥½ã—ã„", "ç´ æ™´ã‚‰ã—ã„", "æœ€é«˜", "è‰¯ã„", "ç¾ã—ã„", "æ–°ã—ã„", "å¿«é©"],
            "negative": ["æ‚²ã—ã„", "è‹¦ã—ã„", "å›°ã‚‹", "æ‚ªã„", "å«Œ", "å¤ã„", "ä¸å¿«"],
            "neutral": ["æ™®é€š", "ã¾ã‚ã¾ã‚", "ãã“ãã“", "ä¸€èˆ¬çš„"]
        }
        
        emotion_scores = {"positive": 0, "negative": 0, "neutral": 0}
        urgency_indicators = 0
        
        for copy in copies:
            # æ„Ÿæƒ…èªã‚«ã‚¦ãƒ³ãƒˆ
            for emotion_type, words in emotion_words.items():
                for word in words:
                    emotion_scores[emotion_type] += copy.count(word)
            
            # ç·Šæ€¥æ€§æŒ‡æ¨™
            urgency_patterns = ['ä»Šã™ã', 'ã™ãã«', 'æ€¥ã„ã§', 'é™å®š', 'ä»Šã ã‘', 'ãŠæ—©ã‚ã«']
            for pattern in urgency_patterns:
                if pattern in copy:
                    urgency_indicators += 1
        
        # ãƒˆãƒ¼ãƒ³åˆ†é¡
        tone_classification = self._classify_tones(copies)
        
        total_emotion_words = sum(emotion_scores.values())
        emotion_ratios = {
            emotion: count / total_emotion_words if total_emotion_words > 0 else 0
            for emotion, count in emotion_scores.items()
        }
        
        return {
            "emotion_scores": emotion_scores,
            "emotion_ratios": emotion_ratios,
            "urgency_score": urgency_indicators / len(copies) if copies else 0,
            "tone_classification": tone_classification,
            "dominant_emotion": max(emotion_ratios.items(), key=lambda x: x[1])[0] if emotion_ratios else "neutral"
        }
    
    def _classify_tones(self, copies: List[str]) -> Dict[str, int]:
        """ãƒˆãƒ¼ãƒ³åˆ†é¡"""
        tones = {
            "formal": 0,      # ä¸å¯§ãƒ»ãƒ•ã‚©ãƒ¼ãƒãƒ«
            "casual": 0,      # ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«
            "urgent": 0,      # ç·Šæ€¥ãƒ»åˆ‡è¿«
            "friendly": 0,    # è¦ªã—ã¿ã‚„ã™ã„
            "professional": 0 # ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«
        }
        
        for copy in copies:
            # ç°¡æ˜“åˆ†é¡
            if any(pattern in copy for pattern in ['ã§ã™', 'ã¾ã™', 'ã”ã–ã„ã¾ã™']):
                tones["formal"] += 1
            
            if any(pattern in copy for pattern in ['ã ã‚ˆ', 'ã ã­', 'ã˜ã‚ƒã‚“', 'ã§ã—ã‚‡']):
                tones["casual"] += 1
            
            if any(pattern in copy for pattern in ['ä»Šã™ã', 'æ€¥ã„ã§', 'é™å®š', 'ãŠæ—©ã‚ã«']):
                tones["urgent"] += 1
            
            if any(pattern in copy for pattern in ['ä¸€ç·’ã«', 'ã¿ã‚“ãªã§', 'ã‚ãªãŸã¨']):
                tones["friendly"] += 1
            
            if any(pattern in copy for pattern in ['åŠ¹æœçš„', 'æœ€é©', 'å®Ÿè¨¼æ¸ˆã¿', 'å°‚é–€']):
                tones["professional"] += 1
        
        return tones
    
    def _extract_dna_patterns(self, vocab: Dict, rhythm: Dict, syntax: Dict, emotion: Dict) -> Dict[str, Any]:
        """DNAãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º"""
        
        # èªå½™ç‰¹æ€§
        vocab_profile = "rich" if vocab["vocabulary_richness"] > 0.7 else \
                       "moderate" if vocab["vocabulary_richness"] > 0.4 else "simple"
        
        # ãƒªã‚ºãƒ ç‰¹æ€§
        rhythm_profile = rhythm["tempo_classification"]
        
        # æ§‹æ–‡ç‰¹æ€§
        syntax_profile = "complex" if syntax["complex_sentence_ratio"] > 0.5 else \
                        "mixed" if syntax["complex_sentence_ratio"] > 0.2 else "simple"
        
        # æ„Ÿæƒ…ç‰¹æ€§
        emotion_profile = emotion["dominant_emotion"]
        
        # å…¨ä½“çš„ãªDNAãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
        dna_signature = f"{vocab_profile}_{rhythm_profile}_{syntax_profile}_{emotion_profile}"
        
        # ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«
        feature_vector = {
            "vocabulary_richness": vocab["vocabulary_richness"],
            "rhythm_tempo": rhythm["syllable_stats"]["avg"],
            "syntax_complexity": syntax["complex_sentence_ratio"],
            "emotion_intensity": max(emotion["emotion_ratios"].values()) if emotion["emotion_ratios"] else 0,
            "urgency_level": emotion["urgency_score"]
        }
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³å¼·åº¦ã‚¹ã‚³ã‚¢
        pattern_strength = {
            "consistency": self._calculate_consistency_score(vocab, rhythm, syntax),
            "distinctiveness": self._calculate_distinctiveness_score(feature_vector),
            "memorability": self._calculate_memorability_score(rhythm, emotion)
        }
        
        return {
            "dna_signature": dna_signature,
            "profiles": {
                "vocabulary": vocab_profile,
                "rhythm": rhythm_profile,
                "syntax": syntax_profile,
                "emotion": emotion_profile
            },
            "feature_vector": feature_vector,
            "pattern_strength": pattern_strength,
            "uniqueness_score": np.mean(list(pattern_strength.values()))
        }
    
    def _calculate_consistency_score(self, vocab: Dict, rhythm: Dict, syntax: Dict) -> float:
        """ä¸€è²«æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        # å„æŒ‡æ¨™ã®å¤‰å‹•ä¿‚æ•°ã®é€†æ•°ã§ä¸€è²«æ€§ã‚’æ¸¬å®š
        cv_vocab = vocab.get("vocabulary_richness", 0)
        cv_rhythm = rhythm["syllable_stats"].get("std", 0) / max(rhythm["syllable_stats"].get("avg", 1), 1)
        cv_syntax = syntax["sentence_length_stats"].get("std", 0) / max(syntax["sentence_length_stats"].get("avg", 1), 1)
        
        avg_cv = (cv_vocab + cv_rhythm + cv_syntax) / 3
        return max(0, 1 - avg_cv)  # CVãŒå°ã•ã„ã»ã©ä¸€è²«æ€§ãŒé«˜ã„
    
    def _calculate_distinctiveness_score(self, feature_vector: Dict) -> float:
        """ç‰¹å¾´æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        # ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã®åˆ†æ•£ã§ç‰¹å¾´æ€§ã‚’æ¸¬å®š
        values = list(feature_vector.values())
        return float(np.std(values)) if values else 0
    
    def _calculate_memorability_score(self, rhythm: Dict, emotion: Dict) -> float:
        """è¨˜æ†¶å®¹æ˜“æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        # ãƒªã‚ºãƒ ã®ç‰¹å¾´æ€§ã¨æ„Ÿæƒ…å¼·åº¦ã§è¨˜æ†¶å®¹æ˜“æ€§ã‚’æ¸¬å®š
        rhythm_score = rhythm.get("alliteration_score", 0)
        emotion_score = max(emotion["emotion_ratios"].values()) if emotion["emotion_ratios"] else 0
        return (rhythm_score + emotion_score) / 2
    
    def _generate_insights(self, basic_stats: Dict, vocab: Dict, dna: Dict) -> List[str]:
        """ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ"""
        insights = []
        
        # åŸºæœ¬çµ±è¨ˆã‹ã‚‰ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
        avg_chars = basic_stats["character_stats"]["avg"]
        if avg_chars < 20:
            insights.append("çŸ­æ–‡ä¸­å¿ƒã®æ§‹æˆã§ã€ç¬é–“çš„ãªã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚’é‡è¦–ã—ã¦ã„ã‚‹")
        elif avg_chars > 50:
            insights.append("é•·æ–‡æ§‹æˆã§ã€è©³ç´°ãªèª¬æ˜ã‚„ç‰©èªæ€§ã‚’é‡è¦–ã—ã¦ã„ã‚‹")
        
        # èªå½™ã‹ã‚‰ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
        vocab_richness = vocab["vocabulary_richness"]
        if vocab_richness > 0.7:
            insights.append("èªå½™ãŒè±Šå¯Œã§ã€è¡¨ç¾åŠ›ã«å¯Œã‚“ã ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°")
        elif vocab_richness < 0.3:
            insights.append("ã‚·ãƒ³ãƒ—ãƒ«ãªèªå½™ã§ã€åˆ†ã‹ã‚Šã‚„ã™ã•ã‚’é‡è¦–ã—ãŸã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°")
        
        # DNAãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
        dna_signature = dna["dna_signature"]
        uniqueness = dna["uniqueness_score"]
        
        if uniqueness > 0.7:
            insights.append(f"ç‹¬ç‰¹ãªã‚³ãƒ”ãƒ¼DNA ({dna_signature}) ã‚’æŒã¡ã€å¼·ã„å€‹æ€§ã‚’è¡¨ç¾ã—ã¦ã„ã‚‹")
        elif uniqueness < 0.3:
            insights.append("æ¨™æº–çš„ãªã‚³ãƒ”ãƒ¼æ§‹é€ ã§ã€å®‰å®šæ€§ã¨è¦ªã—ã¿ã‚„ã™ã•ã‚’é‡è¦–ã—ã¦ã„ã‚‹")
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³å¼·åº¦ã‹ã‚‰ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
        consistency = dna["pattern_strength"]["consistency"]
        if consistency > 0.8:
            insights.append("é«˜ã„ä¸€è²«æ€§ã‚’æŒã¡ã€ãƒ–ãƒ©ãƒ³ãƒ‰ã‚¢ã‚¤ãƒ‡ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒæ˜ç¢º")
        
        distinctiveness = dna["pattern_strength"]["distinctiveness"]
        if distinctiveness > 0.6:
            insights.append("éš›ç«‹ã£ãŸç‰¹å¾´ã‚’æŒã¡ã€ç«¶åˆã¨ã®å·®åˆ¥åŒ–ãŒå›³ã‚‰ã‚Œã¦ã„ã‚‹")
        
        return insights[:5]  # æœ€å¤§5ã¤ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
    
    def save_results(self, result: Dict[str, Any]) -> None:
        """çµæœä¿å­˜"""
        try:
            # JSONè©³ç´°çµæœ
            json_path = os.path.join(self.output_dir, "copy_dna_analysis_result.json")
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2, default=str)
            
            # CSVè¦ç´„çµæœ
            csv_path = os.path.join(self.output_dir, "vocabulary_frequency.csv")
            if result["success"] and "vocabulary_analysis" in result:
                vocab_data = result["vocabulary_analysis"]["top_words"]
                df = pd.DataFrame(list(vocab_data.items()), columns=["Word", "Frequency"])
                df.to_csv(csv_path, index=False, encoding='utf-8')
            
            # å¯è¦–åŒ–
            self._save_visualization(result)
            
            print(f"ğŸ“„ è©³ç´°çµæœ: {json_path}")
            print(f"ğŸ“Š èªå½™é »åº¦: {csv_path}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def _save_visualization(self, result: Dict[str, Any]) -> None:
        """å¯è¦–åŒ–çµæœä¿å­˜"""
        if not result["success"]:
            return
        
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            
            # 1. èªå½™é »åº¦
            vocab_data = result["vocabulary_analysis"]["top_words"]
            if vocab_data:
                words = list(vocab_data.keys())[:10]
                freqs = list(vocab_data.values())[:10]
                axes[0, 0].barh(words, freqs)
                axes[0, 0].set_title("Top 10 èªå½™é »åº¦")
                axes[0, 0].set_xlabel("é »åº¦")
            
            # 2. æ–‡å­—ç¨®åˆ¥æ¯”ç‡
            char_ratios = result["vocabulary_analysis"]["character_type_ratio"]
            if char_ratios:
                labels = list(char_ratios.keys())
                sizes = list(char_ratios.values())
                axes[0, 1].pie(sizes, labels=labels, autopct='%1.1f%%')
                axes[0, 1].set_title("æ–‡å­—ç¨®åˆ¥æ¯”ç‡")
            
            # 3. ãƒˆãƒ¼ãƒ³åˆ†æ
            tone_data = result["emotion_analysis"]["tone_classification"]
            if tone_data:
                tones = list(tone_data.keys())
                counts = list(tone_data.values())
                axes[1, 0].bar(tones, counts)
                axes[1, 0].set_title("ãƒˆãƒ¼ãƒ³åˆ†é¡")
                axes[1, 0].set_ylabel("å‡ºç¾å›æ•°")
                axes[1, 0].tick_params(axis='x', rotation=45)
            
            # 4. DNAç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«
            feature_vector = result["dna_patterns"]["feature_vector"]
            if feature_vector:
                features = list(feature_vector.keys())
                values = list(feature_vector.values())
                axes[1, 1].radar_chart_alternative(features, values)
                axes[1, 1].set_title("DNAç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«")
            
            plt.tight_layout()
            chart_path = os.path.join(self.output_dir, "copy_dna_analysis_chart.png")
            plt.savefig(chart_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"ğŸ“ˆ å¯è¦–åŒ–çµæœ: {chart_path}")
            
        except Exception as e:
            print(f"âš ï¸ å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def print_summary(self, result: Dict[str, Any]) -> None:
        """çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        if not result["success"]:
            print(f"âŒ åˆ†æå¤±æ•—: {result.get('framework_info', {}).get('error', 'Unknown error')}")
            return
        
        print(f"\n{'='*60}")
        print(f"ğŸ§¬ Copy DNA Analysis Summary")
        print(f"{'='*60}")
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯æƒ…å ±
        fw_info = result["framework_info"]
        print(f"ğŸ“¦ Component: {fw_info['component']}")
        print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {fw_info['execution_time']:.2f}ç§’")
        print(f"ğŸ“… å®Ÿè¡Œæ—¥æ™‚: {fw_info['timestamp']}")
        
        # åŸºæœ¬çµ±è¨ˆ
        basic_stats = result["basic_statistics"]
        print(f"\nğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
        print(f"  â€¢ åˆ†æã‚³ãƒ”ãƒ¼æ•°: {basic_stats['total_copies']}ä»¶")
        print(f"  â€¢ å¹³å‡æ–‡å­—æ•°: {basic_stats['character_stats']['avg']:.1f}æ–‡å­—")
        print(f"  â€¢ å¹³å‡æ–‡æ•°: {basic_stats['sentence_stats']['avg']:.1f}æ–‡")
        
        # èªå½™åˆ†æ
        vocab = result["vocabulary_analysis"]
        print(f"\nğŸ”¤ èªå½™åˆ†æ:")
        print(f"  â€¢ èªå½™ã‚µã‚¤ã‚º: {vocab['vocabulary_size']}èª")
        print(f"  â€¢ èªå½™è±Šå¯Œåº¦: {vocab['vocabulary_richness']:.3f}")
        
        # DNAãƒ‘ã‚¿ãƒ¼ãƒ³
        dna = result["dna_patterns"]
        print(f"\nğŸ§¬ DNAãƒ‘ã‚¿ãƒ¼ãƒ³:")
        print(f"  â€¢ DNAç½²å: {dna['dna_signature']}")
        print(f"  â€¢ ç‹¬è‡ªæ€§ã‚¹ã‚³ã‚¢: {dna['uniqueness_score']:.3f}")
        
        profiles = dna['profiles']
        print(f"  â€¢ èªå½™ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«: {profiles['vocabulary']}")
        print(f"  â€¢ ãƒªã‚ºãƒ ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«: {profiles['rhythm']}")
        print(f"  â€¢ æ§‹æ–‡ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«: {profiles['syntax']}")
        print(f"  â€¢ æ„Ÿæƒ…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«: {profiles['emotion']}")
        
        # ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
        insights = result["insights"]
        print(f"\nğŸ’¡ ä¸»è¦ã‚¤ãƒ³ã‚µã‚¤ãƒˆ:")
        for i, insight in enumerate(insights, 1):
            print(f"  {i}. {insight}")
        
        print(f"\nğŸ“ è©³ç´°çµæœã¯ {self.output_dir} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
SAMPLE_COPY_DATA = [
    "æ–°ã—ã„æœã¯ã€æ–°ã—ã„ã‚³ãƒ¼ãƒ’ãƒ¼ã‹ã‚‰å§‹ã¾ã‚‹ã€‚",
    "ä»Šã™ãä½“é¨“ã™ã‚‹ã€ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ãªå‘³ã‚ã„ã€‚",
    "ã‚ãªãŸã®æ¯æ—¥ã‚’ã€ã‚‚ã£ã¨è±Šã‹ã«ã€‚ç‰¹åˆ¥ãªä¸€æ¯ã§ã€‚",
    "é™å®šãƒ–ãƒ¬ãƒ³ãƒ‰ã€ä»Šã ã‘ã®ç‰¹åˆ¥ä¾¡æ ¼ã§ãŠå±Šã‘ã—ã¾ã™ã€‚",
    "é¦™ã‚Šé«˜ã„ã‚³ãƒ¼ãƒ’ãƒ¼ã§ã€ç´ æ™´ã‚‰ã—ã„ä¸€æ—¥ã‚’ã‚¹ã‚¿ãƒ¼ãƒˆã€‚",
    "ã“ã ã‚ã‚Šã®è±†ãŒç”Ÿã¿å‡ºã™ã€æ¥µä¸Šã®å‘³ã‚ã„ã€‚",
    "å¿™ã—ã„æœã«ã‚‚ã€ã»ã£ã¨ä¸€æ¯ã¤ã‘ã‚‹æ™‚é–“ã‚’ã€‚",
    "ä¸–ç•Œæœ€é«˜å³°ã®è¾²åœ’ã‹ã‚‰ã€ç›´æ¥ãŠå±Šã‘ã™ã‚‹å‘³ã€‚"
]

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description='Copy DNA Audit Framework')
    parser.add_argument('--brand', type=str, default='Sample Brand', help='ãƒ–ãƒ©ãƒ³ãƒ‰å')
    parser.add_argument('--output', type=str, default='output', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--copies', nargs='+', help='åˆ†æã™ã‚‹ã‚³ãƒ”ãƒ¼ã®ãƒªã‚¹ãƒˆ')
    
    args = parser.parse_args()
    
    # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯åˆæœŸåŒ–
    framework = CopyDNAFramework(output_base_dir=args.output)
    
    # åˆ†æãƒ‡ãƒ¼ã‚¿æº–å‚™
    copy_samples = args.copies if args.copies else SAMPLE_COPY_DATA
    
    print(f"ğŸ¯ ãƒ–ãƒ©ãƒ³ãƒ‰: {args.brand}")
    print(f"ğŸ“ åˆ†æå¯¾è±¡: {len(copy_samples)}ä»¶ã®ã‚³ãƒ”ãƒ¼")
    
    # åˆ†æå®Ÿè¡Œ
    result = await framework.collect_data(copy_samples, args.brand)
    
    # çµæœä¿å­˜
    framework.save_results(result)
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    framework.print_summary(result)

if __name__ == "__main__":
    asyncio.run(main()) 