#!/usr/bin/env python3
"""
ã‚³ãƒ”ãƒ¼ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ - v6ï¼ˆdemo_5ã®UI + demo_3ã®é«˜å“è³ªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰
demo_5ã®å„ªã‚ŒãŸUIæ©Ÿèƒ½ã¨demo_3ã®å®Ÿç¸¾ã®ã‚ã‚‹é«˜å“è³ªãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµ„ã¿åˆã‚ã›ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³
"""

import streamlit as st
import openai
import pandas as pd
from typing import List, Dict, Tuple
import os
import json
import time

# =========================================
# å…±é€šãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚ªãƒªã‚¨ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ–‡è¨€
# =========================================
DEFAULT_ORIENTATION_TEXT = """ã‚¢ã‚¯ã‚¨ãƒªã‚¢ã‚¹ æ–°ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³é–‹ç™ºã‚ªãƒªã‚¨ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
1. ãƒ–ãƒ©ãƒ³ãƒ‰ã®æ ¹å¹¹ï¼šç§ãŸã¡ã®å­˜åœ¨æ„ç¾©ï¼ˆWhyï¼‰

ã‚¢ã‚¯ã‚¨ãƒªã‚¢ã‚¹ãŒä¿¡ã˜ã‚‹ä¸–ç•Œ

ã€Œã‚¹ãƒãƒ¼ãƒ„ã‚’é€šã˜ã¦äººã€…ãŒè‡ªå·±å®Ÿç¾ã—ã€ã‚ˆã‚Šè‰¯ã„ç¤¾ä¼šã‚’ä½œã‚‹ã€

ç§ãŸã¡ã¯ã€ã‚¹ãƒãƒ¼ãƒ„ãŒå˜ãªã‚‹ç«¶æŠ€ã‚„é‹å‹•ä»¥ä¸Šã®ä¾¡å€¤ã‚’æŒã¤ã¨ä¿¡ã˜ã¦ã„ã¾ã™ã€‚


ãã‚Œã¯ï¼š
è‡ªåˆ†ã®é™ç•Œã«æŒ‘æˆ¦ã—ã€æˆé•·ã™ã‚‹æ©Ÿä¼š
ä»²é–“ã¨å…±ã«ç›®æ¨™ã«å‘ã‹ã†çµ†
åŠªåŠ›ãŒå ±ã‚ã‚Œã‚‹å–œã³
å¥åº·çš„ãªå¿ƒã¨ä½“ã‚’è‚²ã‚€åŸºç›¤

2. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¤ãƒ³ã‚µã‚¤ãƒˆï¼šç¾ä»£ã®ç”Ÿæ´»è€…ãŒæŠ±ãˆã‚‹çœŸå®Ÿ
2025å¹´ã®æ—¥æœ¬ã«ãŠã‘ã‚‹ã€Œã‚¹ãƒãƒ¼ãƒ„ã™ã‚‹äººã€ã®å¤šå±¤çš„ãªå§¿
A. ã‚¢ã‚¹ãƒªãƒ¼ãƒˆå±¤ï¼ˆç«¶æŠ€è€…ï¼‰
è¡¨å±¤ãƒ‹ãƒ¼ã‚ºï¼šãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æœ€å¤§åŒ–ã—ãŸã„ã€ç–²åŠ´ã‚’æ—©ãå›å¾©ã—ãŸã„
æ·±å±¤ãƒ‹ãƒ¼ã‚ºï¼šåŠªåŠ›ãŒå ±ã‚ã‚Œã‚‹ã“ã¨ã‚’ä¿¡ã˜ãŸã„ã€è‡ªåˆ†ã®å¯èƒ½æ€§ã‚’è¨¼æ˜ã—ãŸã„

B. ã‚¦ã‚§ãƒ«ãƒã‚¹å±¤ï¼ˆå¥åº·å¿—å‘ï¼‰
è¡¨å±¤ãƒ‹ãƒ¼ã‚ºï¼šå¥åº·çš„ãªç¿’æ…£ã‚’ç¶šã‘ãŸã„ã€ä½“å‹ã‚’ç¶­æŒã—ãŸã„
æ·±å±¤ãƒ‹ãƒ¼ã‚ºï¼šè‡ªåˆ†ã‚’ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã§ãã¦ã„ã‚‹å®Ÿæ„Ÿã€å°ã•ãªé”æˆæ„Ÿã®ç©ã¿é‡ã­

C. ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£å±¤ï¼ˆã¤ãªãŒã‚Šé‡è¦–ï¼‰
è¡¨å±¤ãƒ‹ãƒ¼ã‚ºï¼šä»²é–“ã¨æ¥½ã—ãé‹å‹•ã—ãŸã„ã€SNSã§ã‚·ã‚§ã‚¢ã—ãŸã„
æ·±å±¤ãƒ‹ãƒ¼ã‚ºï¼šèª°ã‹ã¨ç¹‹ãŒã£ã¦ã„ã‚‹å®Ÿæ„Ÿã€å¿œæ´ã—åˆãˆã‚‹é–¢ä¿‚æ€§

D. ãƒªã‚«ãƒãƒªãƒ¼å±¤ï¼ˆãƒ¡ãƒ³ã‚¿ãƒ«ãƒ˜ãƒ«ã‚¹é‡è¦–ï¼‰
è¡¨å±¤ãƒ‹ãƒ¼ã‚ºï¼šã‚¹ãƒˆãƒ¬ã‚¹è§£æ¶ˆã—ãŸã„ã€ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã—ãŸã„
æ·±å±¤ãƒ‹ãƒ¼ã‚ºï¼šè‡ªåˆ†ã‚’å¤§åˆ‡ã«ã™ã‚‹æ™‚é–“ã€å¿ƒã®ä½™è£•ã‚’å–ã‚Šæˆ»ã™"""

# APIã‚­ãƒ¼è¨­å®šï¼ˆStreamlit Secretså¯¾å¿œï¼‰
try:
    OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
except:
    # ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºæ™‚ã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "sk-proj-your-api-key-here")

# æ®µéšçš„ç”Ÿæˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®šç¾©ï¼ˆdemo_3ã®ã‚·ãƒ³ãƒ—ãƒ«ã§é«˜å“è³ªãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰
STAGED_PROMPTS = [
    {
        "stage": 1,
        "title": "ğŸ¯ æ§‹é€ åŒ–ç”Ÿæˆ",
        "prompt": "ç”Ÿæ´»è€…ã«ã¨ã£ã¦æ–°ã—ã„ä¾¡å€¤ã‚’ç™ºè¦‹ã§ãã‚‹what to say ã‚’ï¼’ï¼æ¡ˆè€ƒãˆã¦ã€ãã®ä¸Šã§äºŒåå€‹ã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆã›ã‚ˆã€‚\n\nâ€»ã€Œwhat to sayã€ã¨ã¯ã€Œãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ä½•ã‹ã€ã‚ã‚‹ã„ã¯ã€ãã®ä¼ç”»ã‚’é€šã—ã¦ã€Œä½•ã‚’æ®‹ã™ã®ã‹ã€ã€Œä½•ã‚’æŒã¡å¸°ã£ã¦ã‚‚ã‚‰ã†ã®ã‹ã€ã¨ã„ã†æ„å‘³ã§ã™ã€‚",
        "description": "20å€‹ã®what to sayã¨20å€‹ã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ"
    },
    {
        "stage": 2,
        "title": "âš¡ å¼·åŒ–ãƒ»æ”¹å–„",
        "prompt": "ã©ã‚Œã‚‚åºƒå‘Šçš„ã§å¿ƒãŒå‹•ã‹ãªã„ã€ã‚‚ã£ã¨å¼·ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå¿…è¦ã€‚ä½¿ã„å¤ã•ã‚ŒãŸè¨€ã„å›ã—ã‚’ä½¿ã‚ãšã«ã€å®šå‹çš„ãªæ§‹æ–‡ã¯é¿ã‘ã¦ã€‚äºŒåå€‹ã®ã‚³ãƒ”ãƒ¼ã‚’è€ƒãˆã¦",
        "description": "ã‚ˆã‚Šå¼·ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«æ”¹å–„ã—ã¾ã™"
    },
    {
        "stage": 3,
        "title": "âœ¨ æœ€çµ‚æ´—ç·´",
        "prompt": "æœ€çµ‚çš„ãªäºŒåå€‹ã®æ¡ˆã‚’ãã‚Œãã‚Œæ„å‘³ãŒå‡ç¸®ã™ã‚‹ã‚ˆã†ã«ã€çŸ­ã„è¨€è‘‰ã«ãƒªãƒ•ãƒ¬ãƒ¼ã‚ºã—ã¦",
        "description": "æ„å‘³ã‚’å‡ç¸®ã—ãŸçŸ­ã„ãƒ•ãƒ¬ãƒ¼ã‚ºã«æœ€çµ‚èª¿æ•´"
    },
    {
        "stage": 4,
        "title": "ğŸ”¥ ã•ã‚‰ãªã‚‹å¼·åŒ–",
        "prompt": "ã©ã‚Œã‚‚åºƒå‘Šçš„ã§å¿ƒãŒå‹•ã‹ãªã„ã€ã‚‚ã£ã¨å¼·ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå¿…è¦ã€‚ä½¿ã„å¤ã•ã‚ŒãŸè¨€ã„å›ã—ã‚’ä½¿ã‚ãšã«ã€å®šå‹çš„ãªæ§‹æ–‡ã¯é¿ã‘ã¦ã€‚äºŒåå€‹ã®ã‚³ãƒ”ãƒ¼ã‚’è€ƒãˆã¦",
        "description": "ã•ã‚‰ã«å¼·ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ"
    }
]

# å„æ®µéšã‚’ãƒ–ãƒ­ãƒƒã‚¯å½¢å¼ã«å¤‰æ›ï¼ˆdemo_5ã®UIã¨ã®äº’æ›æ€§ã®ãŸã‚ï¼‰
COPY_BLOCKS = [
    {
        "id": f"stage_{prompt['stage']}",
        "title": prompt['title'],
        "prompt": prompt['prompt']
    } for prompt in STAGED_PROMPTS
]

# HOW TO SAY å‹å®šç¾©
HOW_TO_SAY_TYPES = [
    {
        "type": 1,
        "name": "æ„å¤–ãªãƒ•ã‚¡ã‚¯ãƒˆã«åŸºã¥ã„ã¦ç™ºè¦‹ã‚’ä¸ãˆã‚‹",
        "examples": ["è½æ›¸ãã‚’ã‚„ã‚ã‚‹ã¨ã€æˆç¸¾ã¯ä¸‹ãŒã‚‹ã€‚", "ã‚»ãƒƒã‚¯ã‚¹ã«ã‚‚ã‚½ãƒƒã‚¯ã‚¹ã‚’ã€‚è¶³å…ƒã‚’æš–ã‚ã‚‹ã¨ã€ã‚ªãƒ«ã‚¬ã‚ºãƒ ã«é”ã—ã‚„ã™ããªã‚‹ã€‚", "æ—¥æœ¬èªã§ã¯ã€äº‹æ•…ã§äº¡ããªã‚‹ã€‚æµ·å¤–ã§ã¯killã¨ã„ã†ã€‚"]
    },
    {
        "type": 2,
        "name": "å»ºå‰ã‚’æ”¾æ£„ã—ã¦æœ¬éŸ³ã‚’èªã‚‹",
        "examples": ["ã¾ãŸå£²ã‚Œãªã‹ã£ãŸã‚‰ã©ã†ã—ã‚ˆã†", "åºƒå‘Šè¦åˆ¶ã«ã‚ˆã‚Šã€ã‚µãƒ³ãƒã‚’æŒãŸã•ã‚Œã¦ã„ã¾ã™"]
    },
    {
        "type": 3,
        "name": "å•†å“ä¾¡å€¤ã‚’æœ€å¤§åŒ–ã—ã¦ã€ç¤¾ä¼šã«ãŠã‘ã‚‹æ„å‘³ã‚’èªã‚‹",
        "examples": ["ãƒ­ã‚±ãƒƒãƒˆã‚‚ã€æ–‡æˆ¿å…·ã‹ã‚‰ç”Ÿã¾ã‚ŒãŸã€‚", "è‹±èªã‚’è©±ã›ã‚‹ã¨ã€10å„„äººã¨è©±ã›ã‚‹", "åœ°å›³ã«æ®‹ã‚‹ä»•äº‹ã€‚ï¼ˆå»ºè¨­ä¼šç¤¾ã®ã‚³ãƒ”ãƒ¼ï¼‰"]
    },
    {
        "type": 4,
        "name": "æ•°ãˆæ–¹ã‚’å·¥å¤«ã—ã¦ã¿ã‚‹",
        "examples": ["æ—¥æœ¬ã§ï¼”ï¼—ç•ªç›®ã«æœ‰åãªçœŒ", "å››åæ­³ã¯ï¼’åº¦ç›®ã®ãƒã‚¿ãƒã€‚", "ï¼‘å„„ä½¿ã£ã¦ã‚‚ã€ã¾ã ï¼’å„„ã€‚"]
    },
    {
        "type": 5,
        "name": "ç‰©äº‹ã‚’æ‰ãˆã‚‹è¦–ç‚¹ã‚’å¤‰ãˆã¦ã¿ã‚‹",
        "examples": ["ã¼ãã®ãŠçˆ¶ã•ã‚“ã¯ã€æ¡ƒå¤ªéƒã¨ã„ã†ã‚„ã¤ã«æ®ºã•ã‚Œã¾ã—ãŸã€‚ï¼ˆé¬¼è¦–ç‚¹ï¼‰", "ãŠã—ã‚Šã ã£ã¦æ´—ã£ã¦ã»ã—ã„ï¼ˆèº«ä½“è¦–ç‚¹ï¼‰", "å¤ªé™½ã‹ã‚‰è¦‹ã‚Œã°ã€æ—¥æœ¬ã«ã¯åºƒå¤§ãªç©ºãåœ°ãŒåºƒãŒã£ã¦ã„ã‚‹ï¼ˆå¤ªé™½è¦–ç‚¹ï¼‰"]
    },
    {
        "type": 6,
        "name": "æ–°ã—ã„äºŒé …å¯¾ç«‹ã‚’ä½œã£ã¦ã¿ã‚‹",
        "examples": ["ã‚´ãƒªãƒãƒƒãƒãƒ§ã€‚ç´°ãƒãƒƒãƒãƒ§ã€‚", "ã²ãŸãƒ‘ãƒ³ã€ã¤ã‘ãƒ‘ãƒ³", "æ¨©åŠ›ã‚ˆã‚Šã€æ„›ã ã­"]
    },
    {
        "type": 7,
        "name": "å•†å“ãŒãªã„ã“ã¨ã«ã‚ˆã‚‹ä¸ä¾¿ã‚’æã",
        "examples": ["éƒ¨é•·ã®å±±æœ¬ã¯ã¾ã‚‚ãªãæˆ»ã‚Šã¾ã™ã®ã§ã€ãã“ã«åº§ã£ã¦ã‚ã€‚ï¼ˆè‹±ä¼šè©±ã®ã‚³ãƒ”ãƒ¼ï¼‰", "ä»Šã€ã“ã®ç”·ã®ã‚«ãƒãƒ³ã®ä¸­ã§ã¯ã€ æ°´ç­’ã®ãŠèŒ¶ãŒã‚ã¡ã‚ƒãã¡ã‚ƒæ¼ã‚Œã¦ã„ã‚‹ã€‚"]
    },
    {
        "type": 8,
        "name": "ã»ã£ã“ã‚Šã™ã‚‹ã‚·ãƒ¼ãƒ³ã‚’åˆ‡ã‚Šå–ã‚‹",
        "examples": ["é‡‘é­šã®ä¾¿ç§˜ãªãŠã‚‹ã€‚ï¼ˆæ°´æ—é¤¨ã®ã‚³ãƒ”ãƒ¼ï¼‰", "ã‚‚ã†ä¸€å›ã¾ãŸãŒã£ã¦ã‹ã‚‰å¯ã‚ˆã€‚ï¼ˆãƒã‚¤ã‚¯ã®ã‚³ãƒ”ãƒ¼ï¼‰", "ã‚¹ã‚­ãƒƒãƒ—ã—ã¡ã‚ƒã£ãŸã€‚ï¼ˆã‚³ãƒ¼ã‚¸ãƒ¼ã‚³ãƒ¼ãƒŠãƒ¼ã®ã‚³ãƒ”ãƒ¼ï¼‰"]
    },
    {
        "type": 9,
        "name": "ä¼æ¥­åã‚’äººã®åå‰ã®ã‚ˆã†ã«ä½¿ã†",
        "examples": ["ãªãœã¤ã°ã•ã‚’ä½¿ã‚ãªã„ã‚“ã ã€‚ã‚ãªãŸã®è³‡ç”£ã‚‚ãã†ã§ã™ã€‚ã¤ã°ã•è¨¼åˆ¸", "æ¥½å¤©ã‚«ãƒ¼ãƒ‰ãƒãƒ³", "æ–°ã—ã„è‹±é›„ã€å§‹ã¾ã‚‹ã€‚au"]
    },
    {
        "type": 10,
        "name": "ãã®æ™‚ä»£ãªã‚‰ã§ã¯ã®ç¤¾ä¼šèª²é¡Œã‚’èªã‚‹",
        "examples": ["åŒæ€§ã‚’å¥½ãã«ãªã‚‹ã®ã¯ã€ã˜ã¤ã¯å·¦åˆ©ãã¨åŒã˜ãã‚‰ã„ã„ã‚‹ã€‚", "å¹´ã‚’ã¨ã‚‹ã ã‘ã§ã€åŠ£åŒ–ã¨å‘¼ã°ã‚Œã‚‹æ™‚ä»£ã‚’ç”Ÿãã¦ã„ã‚‹ã€‚", "æ—¥æœ¬åˆã®å¥³æ€§ç·ç†ã¯ã€ãã£ã¨ã‚‚ã†ã€ã“ã®ä¸–ã«ã„ã‚‹ã€‚"]
    },
    {
        "type": 11,
        "name": "ç´å¾—ã§ãã‚‹ä¸–ã®ä¸­ã®æ³•å‰‡ã‚’ä¼ãˆã‚‹",
        "examples": ["ãŠæ¯ã•ã‚“ã‚’è‚²ã¦ã‚‹ã®ã¯ã€èµ¤ã¡ã‚ƒã‚“ã§ã™ã€‚", "èŠ±ã‚’è‚²ã¦ã‚‹ã‚ˆã†ã«ãªã‚‹ã¨é›¨ãŒå¥½ãã«ãªã‚‹", "è©¦ç€å®¤ã§æ€ã„å‡ºã—ãŸã‚‰ã€æœ¬æ°—ã®æ‹ã ã¨æ€ã†ã€‚", "ç€ç‰©ã‚’ç€ã¦ã„ã‚‹æ—¥ã¯ã€ã™ã“ã—ä¸å¯§ã«ç”Ÿãã¦ã„ã‚‹ã€‚"]
    },
    {
        "type": 12,
        "name": "å¿ƒã®å£°ã‚„ã¤ã¶ã‚„ãã‚’ã‚³ãƒ”ãƒ¼ã«ã™ã‚‹",
        "examples": ["ç§ã ã‘ã€ç¾äººã ã£ãŸã‚‰ã€ã„ã„ã®ã«ã€‚", "ãã†ã ã€€äº¬éƒ½ã€ã„ã“ã†ã€‚", "ã¤ã¾ã‚‰ã‚“ï¼"]
    },
    {
        "type": 13,
        "name": "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãŒå…±æ„Ÿã§ãã‚‹æƒ³ã„ã‚’ä»£å¼ã—ã¦ã‚ã’ã‚‹",
        "examples": ["çŸ¥ååº¦ã ã‘ãŒä¸€æµã®ä¼šç¤¾ã§åƒãã‚ˆã‚Šã€çŸ¥ååº¦ã ã‘ãŒäºŒæµã®ä¼šç¤¾ã§åƒããŸã„ã€‚", "çµå©šã—ãªãã¦ã‚‚å¹¸ã›ã«ãªã‚Œã‚‹ã“ã®æ™‚ä»£ã«ã€€ç§ã¯ã€ã‚ãªãŸã¨çµå©šã—ãŸã„ã®ã§ã™ã€‚", "ï¼‘å¹´ãŒéãã‚‹ã®ã¯æ—©ã„ãŒã€ï¼‘æ—¥ã¯ãªã‹ãªã‹çµ‚ã‚ã‚‰ãªã„ã€‚"]
    },
    {
        "type": 14,
        "name": "è‡ªè™çš„ã«è‡ªåˆ†è‡ªèº«ã‚’èªã‚‹",
        "examples": ["ã“ã“ã¯ã€æ—¥æœ¬ä¸€å¿ƒã®è·é›¢ãŒé ã„ã‚µãƒ•ã‚¡ãƒªãƒ‘ãƒ¼ã‚¯", "ã‚¹ã‚¤ã¦ã¾ã™åµå±±", "ãã®ç¨‹åº¦ã®æ©Ÿèƒ½ãªã‚‰ãƒ‰ãƒ³ã‚­ã§ååˆ†ã ï¼"]
    },
    {
        "type": 15,
        "name": "ãƒ€ã‚¸ãƒ£ãƒ¬ã«ã—ã¦ã¿ã‚‹",
        "examples": ["ãƒã‚¶ãƒ¼ãƒ«ã§ã”ã–ãƒ¼ã‚‹", "ã§ã£ã‹ã„ã©ãŠã€‚åŒ—æµ·é“", "ãƒŠã‚¤ãƒ•ã®ã‚ˆã†ãªãƒŠã‚¤ãƒ¼ãƒ–ã€‚", "ã‚ã—ãŸã®ã‚‚ã¨ å‘³ã®ç´ ", "ã‚«ãƒ©ãƒ€ã«ãƒ”ãƒ¼ã‚¹ã€‚ã‚«ãƒ«ãƒ”ã‚¹"]
    },
    {
        "type": 16,
        "name": "æˆåˆ†ã®ã‚ˆã†ã«è¡¨ç¾ã—ã¦ã¿ã‚‹",
        "examples": ["ãƒãƒ•ã‚¡ãƒªãƒ³ã®åŠåˆ†ã¯ã‚„ã•ã—ã•ã§ã§ãã¦ã¾ã™ã€‚", "ãŠã„ã—ã„ã‚‚ã®ã¯ã€è„‚è‚ªã¨ç³–ã§ã§ãã¦ã„ã‚‹", "ä¸–ç•Œã¯èª°ã‹ã®ä»•äº‹ã§ã§ãã¦ã„ã‚‹"]
    },
    {
        "type": 17,
        "name": "ä¾¡å€¤ã‚’å†å®šç¾©ã™ã‚‹",
        "examples": ["å¹´è³€çŠ¶ã¯ã€è´ˆã‚Šç‰©ã ã¨æ€ã†ã€‚", "æˆ¦äº‰ã‚’ï¼”åº¦ã‚‚çµŒé¨“ã—ãŸã‚¸ãƒ¼ãƒ³ã‚ºï¼ˆå¤ç€ã®ã‚³ãƒ”ãƒ¼ï¼‰", "ãƒãƒ§ã‚³ãŒç¾©ç†ãªã‚‰ã€ã‚¢ãƒ¡ã¯äººæƒ…ã€‚", "æ˜ ç”»ã¯ã€æœ¬å½“ã®ã“ã¨ã‚’è¨€ã†å˜˜ã ã€‚"]
    },
    {
        "type": 18,
        "name": "åŠ¹æœã‚’ä¼ãˆã‚‹",
        "examples": ["å€’ã‚Œã‚‹ã ã‘ã§è…¹ç­‹", "å¸å¼•åŠ›ãŒå¤‰ã‚ã‚‰ãªã„ãŸã ä¸€ã¤ã®æƒé™¤æ©Ÿ", "ã‚ã‚‹æ—¥ã€æ—¥çµŒã¯é¡”ã«å‡ºã‚‹"]
    },
    {
        "type": 19,
        "name": "ãƒ©ã‚¤ãƒãƒ«ã«å–§å˜©ã‚’å£²ã‚‹",
        "examples": ["ãƒãƒ¨ãƒãƒ¼ã‚ºã‚ˆã€‚çœŸä¼¼ã™ã‚“ãªã‚ˆã€‚ï¼ˆã‚±ãƒãƒ£ãƒƒãƒ—ã®ã‚³ãƒ”ãƒ¼ï¼‰", "ç¾é‡‘ã£ã¦ã€å¥‡å¦™ãªãƒ¢ãƒã‚’æŒã¡æ­©ã„ã¦ã„ã‚‹ã‚‚ã‚“ã ï¼ˆã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã®ã‚³ãƒ”ãƒ¼ï¼‰"]
    },
    {
        "type": 20,
        "name": "å¸¸è­˜ã‚’ã²ã£ãã‚Šè¿”ã—ã¦ã¿ã‚‹",
        "examples": ["åœ°å‘³ãƒãƒ­ã‚¦ã‚£ãƒ³", "æŠ½é¸ã§ï¼‘åã‚’ãƒã‚ºãƒ¬ã¨ã™ã‚‹", "å¥åº·ãŒãƒ–ãƒ¼ãƒ ã«ãªã‚‹ãªã‚“ã¦ã€ç•°å¸¸ã ã€‚"]
    }
]

def get_available_models() -> List[str]:
    """åˆ©ç”¨å¯èƒ½ãªOpenAIãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—"""
    try:
        openai.api_key = OPENAI_API_KEY
        models = openai.models.list()
        # GPTãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆãƒãƒ£ãƒƒãƒˆç”¨ï¼‰
        gpt_models = []
        for model in models.data:
            model_id = model.id
            if any(prefix in model_id.lower() for prefix in ['gpt-', 'o1-', 'o3-']):
                gpt_models.append(model_id)
        
        # ã‚½ãƒ¼ãƒˆã—ã¦è¿”ã™
        return sorted(gpt_models, reverse=True)
    except Exception as e:
        st.sidebar.error(f"ãƒ¢ãƒ‡ãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’è¿”ã™
        return get_default_models()

def get_default_models() -> List[str]:
    """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆ"""
    return [
        "gpt-4.1",
        "gpt-4o",
        "gpt-4o-mini"
    ]

def get_model_categories() -> Dict[str, Dict]:
    """ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«æ•´ç†"""
    return {
        "ğŸ¯ æ¨™æº–GPTãƒ¢ãƒ‡ãƒ«ï¼ˆæ¨å¥¨ï¼‰": {
            "gpt-4o": {
                "price": "$2.50/$10",
                "description": "ğŸ¯ æœ€æ–°GPT-4ï¼ˆãƒãƒ©ãƒ³ã‚¹å‹ãƒ»å®‰å®šï¼‰",
                "use_case": "ä¸€èˆ¬çš„ãªã‚³ãƒ”ãƒ¼ç”Ÿæˆ",
                "recommended": True,
                "note": "âœ… Chat Completions APIå¯¾å¿œ (å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)"
            },
            "gpt-4o-mini": {
                "price": "$0.15/$0.60",
                "description": "ğŸ’¨ è»½é‡ç‰ˆGPT-4ï¼ˆé«˜é€Ÿãƒ»ä½ã‚³ã‚¹ãƒˆï¼‰",
                "use_case": "å¤§é‡å‡¦ç†ã€é«˜é€Ÿç”Ÿæˆ",
                "recommended": True,
                "note": "âœ… Chat Completions APIå¯¾å¿œ (å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)"
            },
            "gpt-4.1": {
                "price": "ä¾¡æ ¼æœªå…¬é–‹",
                "description": "ğŸ†• æ¬¡ä¸–ä»£GPTï¼ˆãƒ™ãƒ¼ã‚¿ç‰ˆï¼‰",
                "use_case": "æœ€æ–°æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ",
                "recommended": False,
                "note": "âš ï¸ ãƒ™ãƒ¼ã‚¿ç‰ˆãƒ»è¦æ¤œè¨¼"
            }
        }
    }

def display_model_selector() -> Tuple[str, str]:
    """ãƒ¢ãƒ‡ãƒ«é¸æŠUIã‚’è¡¨ç¤º"""
    st.sidebar.header("ğŸ¤– AIãƒ¢ãƒ‡ãƒ«è¨­å®š")
    
    # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
    available_models = get_available_models()
    model_categories = get_model_categories()
    
    # ãƒ¢ãƒ‡ãƒ«é¸æŠæ–¹å¼ã‚’é¸ã¶
    selection_mode = st.sidebar.radio(
        "é¸æŠæ–¹å¼",
        ["ã‚«ãƒ†ã‚´ãƒªåˆ¥é¸æŠ", "å…¨ãƒ¢ãƒ‡ãƒ«ä¸€è¦§"],
        help="æ¨å¥¨ã¯ã‚«ãƒ†ã‚´ãƒªåˆ¥é¸æŠã§ã™"
    )
    
    selected_model = None
    model_info = ""
    
    if selection_mode == "ã‚«ãƒ†ã‚´ãƒªåˆ¥é¸æŠ":
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥é¸æŠ
        st.sidebar.markdown("### ğŸ“‹ ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ†ã‚´ãƒª")
        
        for category, models in model_categories.items():
            with st.sidebar.expander(category, expanded=True):
                for model_id, model_data in models.items():
                    if model_id in available_models:
                        # æ¨å¥¨ãƒãƒ¼ã‚¯ã‚’è¿½åŠ 
                        display_name = f"â­ {model_id}" if model_data.get('recommended') else model_id
                        
                        if st.button(
                            f"{display_name}",
                            key=f"select_{model_id}",
                            help=f"{model_data['description']}\nä¾¡æ ¼: {model_data['price']}\nç”¨é€”: {model_data['use_case']}"
                        ):
                            selected_model = model_id
                            model_info = model_data['description']
                            st.session_state.selected_model = model_id
                            st.session_state.model_info = model_info
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
        if 'selected_model' in st.session_state:
            selected_model = st.session_state.selected_model
            model_info = st.session_state.get('model_info', '')
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé¸æŠ
            if "gpt-4.1" in available_models:
                selected_model = "gpt-4.1"
                model_info = "ğŸ†• æ¬¡ä¸–ä»£GPTï¼ˆãƒ™ãƒ¼ã‚¿ç‰ˆï¼‰"
            elif "gpt-4o" in available_models:
                selected_model = "gpt-4o"
                model_info = "ğŸ¯ æœ€æ–°GPT-4ï¼ˆãƒãƒ©ãƒ³ã‚¹å‹ãƒ»å®‰å®šï¼‰"
            elif "gpt-4o-mini" in available_models:
                selected_model = "gpt-4o-mini"
                model_info = "ğŸ’¨ è»½é‡ç‰ˆGPT-4ï¼ˆé«˜é€Ÿãƒ»ä½ã‚³ã‚¹ãƒˆï¼‰"
            else:
                selected_model = available_models[0]
                model_info = "é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®è©³ç´°æƒ…å ±"
            st.session_state.selected_model = selected_model
            st.session_state.model_info = model_info
    
    else:
        # å…¨ãƒ¢ãƒ‡ãƒ«ä¸€è¦§é¸æŠ
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ±ºå®š
        default_index = 0
        if "gpt-4.1" in available_models:
            default_index = available_models.index("gpt-4.1")
        elif "gpt-4o" in available_models:
            default_index = available_models.index("gpt-4o")
        elif "gpt-4o-mini" in available_models:
            default_index = available_models.index("gpt-4o-mini")
        
        selected_model = st.sidebar.selectbox(
            "ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
            available_models,
            index=default_index
        )
        
        # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã‚’æ¤œç´¢
        for category, models in model_categories.items():
            if selected_model in models:
                model_info = models[selected_model]['description']
                break
        else:
            model_info = "é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®è©³ç´°æƒ…å ±"
    
    # ç¾åœ¨é¸æŠã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’è¡¨ç¤º
    if selected_model:
        st.sidebar.success(f"âœ… é¸æŠä¸­: **{selected_model}**")
        st.sidebar.info(model_info)
        
        # ä¾¡æ ¼æƒ…å ±ã‚’è¡¨ç¤º
        for category, models in model_categories.items():
            if selected_model in models:
                price_info = models[selected_model]['price']
                use_case = models[selected_model]['use_case']
                note_info = models[selected_model].get('note', '')
                st.sidebar.markdown(f"ğŸ’° **ä¾¡æ ¼**: {price_info}")
                st.sidebar.markdown(f"ğŸ¯ **é©ç”¨ä¾‹**: {use_case}")
                if note_info:
                    st.sidebar.markdown(f"â„¹ï¸ **çŠ¶æ…‹**: {note_info}")
                break
    
    return selected_model, model_info

def load_conversation_history(csv_file: str = None) -> List[Dict[str, str]]:
    """CSVã‹ã‚‰ä¼šè©±å±¥æ­´ã‚’èª­ã¿è¾¼ã¿ã€OpenAI APIå½¢å¼ã«å¤‰æ›"""
    if not csv_file:
        return []
        
    try:
        df = pd.read_csv(csv_file)
        # NaNå€¤ã‚’å‰Šé™¤
        df = df.dropna()
        messages = []
        
        for _, row in df.iterrows():
            role = "user" if row['side'] == 'human' else "assistant"
            content = str(row['prompt']).strip()
            
            # ç©ºæ–‡å­—åˆ—ã‚„NaNã‚’ã‚¹ã‚­ãƒƒãƒ—
            if content and content != 'nan':
                messages.append({"role": role, "content": content})
        
        return messages
    except FileNotFoundError:
        st.warning(f"{csv_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å±¥æ­´ãªã—ã§ç¶šè¡Œã—ã¾ã™ã€‚")
        return []
    except Exception as e:
        st.error(f"å±¥æ­´èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

def parse_json_response(response_text: str) -> Dict:
    """JSONå½¢å¼ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’è¡Œã†"""
    try:
        # JSONãƒ–ãƒ­ãƒƒã‚¯ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®æŠ½å‡º
        if "```json" in response_text:
            json_start = response_text.find("```json") + 7
            json_end = response_text.find("```", json_start)
            json_text = response_text[json_start:json_end].strip()
        elif "{" in response_text and "}" in response_text:
            # JSONéƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
            json_start = response_text.find("{")
            json_end = response_text.rfind("}") + 1
            json_text = response_text[json_start:json_end]
        else:
            json_text = response_text
            
        parsed_data = json.loads(json_text)
        return parsed_data
    except json.JSONDecodeError as e:
        # JSONè§£æå¤±æ•—æ™‚ã®ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
        st.error(f"JSONè§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
        with st.expander("âš ï¸ JSONè§£æã‚¨ãƒ©ãƒ¼ã®è©³ç´°", expanded=False):
            st.text("ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆæœ€åˆã®500æ–‡å­—ï¼‰:")
            st.text(response_text[:500])
            st.text("æŠ½å‡ºã—ãŸJSONéƒ¨åˆ†ï¼ˆæœ€åˆã®500æ–‡å­—ï¼‰:")
            if 'json_text' in locals():
                st.text(json_text[:500])
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ†ã‚­ã‚¹ãƒˆã‚’ãã®ã¾ã¾è¿”ã™
        return {"copies": [response_text], "error": "JSONè§£æã«å¤±æ•—ã—ã¾ã—ãŸ"}

def format_copies_display(parsed_json: Dict) -> str:
    """ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸJSONã‹ã‚‰ã‚³ãƒ”ãƒ¼ã®ã¿ã‚’æŠ½å‡ºã—ã¦è¡¨ç¤ºç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    if "error" in parsed_json:
        return parsed_json.get("copies", ["ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"])[0]
    
    copies = []
    
    # æ§˜ã€…ãªJSONæ§‹é€ ã«å¯¾å¿œ
    if "copies" in parsed_json:
        if isinstance(parsed_json["copies"], list):
            copies = parsed_json["copies"]
        else:
            copies = [str(parsed_json["copies"])]
    elif "results" in parsed_json:
        if isinstance(parsed_json["results"], list):
            copies = parsed_json["results"]
        else:
            copies = [str(parsed_json["results"])]
    elif "what_to_say" in parsed_json and "copies" in parsed_json:
        # æ®µéš1ã®æ§‹é€ åŒ–ç”Ÿæˆç”¨
        what_to_say = parsed_json.get("what_to_say", [])
        copy_list = parsed_json.get("copies", [])
        
        formatted = "ã€What to Sayï¼ˆ20æ¡ˆï¼‰ã€‘\n"
        for i, wts in enumerate(what_to_say[:20], 1):
            formatted += f"{i}. {wts}\n"
        
        formatted += "\nã€ã‚³ãƒ”ãƒ¼ï¼ˆ20æ¡ˆï¼‰ã€‘\n"
        for i, copy in enumerate(copy_list[:20], 1):
            formatted += f"{i}. {copy}\n"
        
        return formatted
    elif "refinements" in parsed_json:
        # How to Sayæ´—ç·´ç”¨ - æ´—ç·´å¾Œã®ã‚³ãƒ”ãƒ¼ã®ã¿è¡¨ç¤ºï¼ˆç•ªå·ãªã—ï¼‰
        refinements = parsed_json["refinements"]
        copies = []
        for refinement in refinements:
            copy = refinement.get("copy", "")
            if copy:
                copies.append(copy)
        
        # ã‚·ãƒ³ãƒ—ãƒ«ã«æ”¹è¡ŒåŒºåˆ‡ã‚Šã§è¡¨ç¤º
        if copies:
            return "\n".join(copies)
        else:
            return "æ´—ç·´ã•ã‚ŒãŸã‚³ãƒ”ãƒ¼ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"
    else:
        # ãã®ä»–ã®æ§‹é€ ã®å ´åˆã€å…¨ä½“ã‚’æ–‡å­—åˆ—åŒ–
        copies = [str(parsed_json)]
    
    # ãƒªã‚¹ãƒˆå½¢å¼ã®å ´åˆã¯ç•ªå·ä»˜ãã§è¡¨ç¤º
    if len(copies) > 1:
        return "\n".join([f"{i}. {copy}" for i, copy in enumerate(copies, 1)])
    else:
        return copies[0] if copies else "ã‚³ãƒ”ãƒ¼ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"

def extract_copies_list(parsed_json: Dict) -> List[str]:
    """ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸJSONã‹ã‚‰ã‚³ãƒ”ãƒ¼ã®ãƒªã‚¹ãƒˆã‚’æŠ½å‡º"""
    copies = []
    
    if "error" in parsed_json:
        return []
    
    # æ§˜ã€…ãªJSONæ§‹é€ ã«å¯¾å¿œ
    if "copies" in parsed_json:
        if isinstance(parsed_json["copies"], list):
            copies = parsed_json["copies"]
        else:
            copies = [str(parsed_json["copies"])]
    elif "results" in parsed_json:
        if isinstance(parsed_json["results"], list):
            copies = parsed_json["results"]
        else:
            copies = [str(parsed_json["results"])]
    elif "what_to_say" in parsed_json and "copies" in parsed_json:
        # æ®µéš1ã®æ§‹é€ åŒ–ç”Ÿæˆç”¨ - copiesã®ã¿æŠ½å‡º
        copy_list = parsed_json.get("copies", [])
        copies = copy_list
    elif "refinements" in parsed_json:
        # How to Sayæ´—ç·´ç”¨ - æ´—ç·´å¾Œã®ã‚³ãƒ”ãƒ¼ã®ã¿æŠ½å‡º
        refinements = parsed_json["refinements"]
        for refinement in refinements:
            copy = refinement.get("copy", "")
            if copy:
                copies.append(copy)
    else:
        # ãã®ä»–ã®æ§‹é€ ã®å ´åˆã€å…¨ä½“ã‚’æ–‡å­—åˆ—ã¨ã—ã¦æ‰±ã†
        copies = [str(parsed_json)]
    
    return copies

def generate_feedback_based_copy(orientation: str, good_copies: List[str], bad_copies: List[str], conversation_messages: List[Dict], model: str = "gpt-4o", temperature: float = 0.9) -> str:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’åŸºã«ã—ãŸè‡ªçœçš„ã‚³ãƒ”ãƒ¼ç”Ÿæˆ"""
    openai.api_key = OPENAI_API_KEY
    
    # ãƒ¢ãƒ‡ãƒ«ç¨®åˆ¥ã‚’åˆ¤å®š
    is_o1_pro = "o1-pro" in model.lower()
    is_o3_or_o1_other = any(prefix in model.lower() for prefix in ['o1-', 'o3-']) and not is_o1_pro
    use_json_mode = supports_json_mode(model)
    
    # è‡ªçœãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
    good_copies_text = "\n".join([f"âœ… {copy}" for copy in good_copies])
    bad_copies_text = "\n".join([f"âŒ {copy}" for copy in bad_copies])
    
    self_reflection_prompt = f"""
ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æã€‘

ä»¥ä¸‹ã®ã‚³ãƒ”ãƒ¼ã«ã¤ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé¸æŠã—ãŸè‰¯ã„ã‚³ãƒ”ãƒ¼ã¨é¸æŠã—ãªã‹ã£ãŸæ‚ªã„ã‚³ãƒ”ãƒ¼ã‚’åˆ†æã—ã¦ãã ã•ã„ï¼š

ã€è‰¯ã„ã‚³ãƒ”ãƒ¼ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé¸æŠï¼‰ã€‘
{good_copies_text}

ã€æ‚ªã„ã‚³ãƒ”ãƒ¼ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé¸æŠã—ãªã‹ã£ãŸï¼‰ã€‘
{bad_copies_text}

ã€åˆ†æã‚¿ã‚¹ã‚¯ã€‘
1. è‰¯ã„ã‚³ãƒ”ãƒ¼ã¨æ‚ªã„ã‚³ãƒ”ãƒ¼ã®é•ã„ã‚’è©³ç´°ã«åˆ†æã—ã¦ãã ã•ã„
2. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè©•ä¾¡ã—ãŸè¦ç´ ï¼ˆè¡¨ç¾ã€éŸ¿ãã€å°è±¡ã€åŠ¹æœãªã©ï¼‰ã‚’ç‰¹å®šã—ã¦ãã ã•ã„
3. ãã®åˆ†æçµæœã‚’è¸ã¾ãˆã¦ã€ã‚ˆã‚Šè‰¯ã„ã‚³ãƒ”ãƒ¼ã‚’20å€‹æ–°ãŸã«ç”Ÿæˆã—ã¦ãã ã•ã„

ã€åˆ†æè¦³ç‚¹ã€‘
- è¨€è‘‰ã®é¸ã³æ–¹
- æ„Ÿæƒ…çš„ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ
- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¸ã®éŸ¿ãæ–¹
- è¨˜æ†¶ã«æ®‹ã‚Šã‚„ã™ã•
- ç‹¬å‰µæ€§
- èª¬å¾—åŠ›
- ç°¡æ½”ã•

åˆ†æçµæœã‚’æ´»ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ã«åˆè‡´ã™ã‚‹æ–°ã—ã„ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
"""
    
    json_instruction = """

å›ç­”ã¯å¿…ãšJSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã®å½¢å¼ã«å¾“ã£ã¦ãã ã•ã„ï¼š

{
  "analysis": "è‰¯ã„ã‚³ãƒ”ãƒ¼ã¨æ‚ªã„ã‚³ãƒ”ãƒ¼ã®é•ã„ã®åˆ†æçµæœ",
  "insights": [
    "æ´å¯Ÿ1",
    "æ´å¯Ÿ2",
    ...
  ],
  "copies": [
    "æ”¹å–„ã•ã‚ŒãŸã‚³ãƒ”ãƒ¼1",
    "æ”¹å–„ã•ã‚ŒãŸã‚³ãƒ”ãƒ¼2",
    ...ï¼ˆ20å€‹ï¼‰
  ]
}

JSONä»¥å¤–ã®èª¬æ˜ã‚„å‰ç½®ãã¯ä¸€åˆ‡å«ã‚ãšã€ç´”ç²‹ãªJSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""
    
    try:
        if is_o1_pro:
            if conversation_messages:
                conversation_text = "\n\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_messages])
                user_message = f"{orientation}\n\néå»ã®ä¼šè©±:\n{conversation_text}\n\n{self_reflection_prompt}{json_instruction}"
            else:
                user_message = f"{orientation}\n\n{self_reflection_prompt}{json_instruction}"
            
            response = openai.responses.create(
                model=model,
                input=user_message,
                reasoning={"effort": "high"}
            )
            response_text = response.choices[0].message.content
            
        elif is_o3_or_o1_other:
            if conversation_messages:
                conversation_text = "\n\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_messages])
                user_message = f"{orientation}\n\néå»ã®ä¼šè©±:\n{conversation_text}\n\n{self_reflection_prompt}{json_instruction}"
            else:
                user_message = f"{orientation}\n\n{self_reflection_prompt}{json_instruction}"
            
            messages = [{"role": "user", "content": user_message}]
            
            response = openai.chat.completions.create(
                model=model,
                messages=messages,
                max_completion_tokens=3000
            )
            response_text = response.choices[0].message.content
        
        else:
            system_message = {
                "role": "system", 
                "content": f"""ã‚ãªãŸã¯å„ªç§€ãªã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è©³ç´°ã«åˆ†æã—ã€ãã®æ´å¯Ÿã‚’æ´»ç”¨ã—ã¦ã‚ˆã‚Šè‰¯ã„ã‚³ãƒ”ãƒ¼ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
è‰¯ã„ã‚³ãƒ”ãƒ¼ã¨æ‚ªã„ã‚³ãƒ”ãƒ¼ã®é•ã„ã‚’æ·±ãç†è§£ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ã«åˆè‡´ã™ã‚‹æ”¹å–„ã•ã‚ŒãŸã‚³ãƒ”ãƒ¼ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

é‡è¦ï¼šç”Ÿæˆã™ã‚‹ã‚³ãƒ”ãƒ¼ã¯ç´”ç²‹ãªã‚³ãƒ”ãƒ¼æ–‡è¨€ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚èª¬æ˜æ–‡ã€åˆ†æã€å‹ç•ªå·ã€å‹åãªã©ã®ä½™è¨ˆãªæƒ…å ±ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚ã‚³ãƒ”ãƒ¼ã¯å®Œæˆå“ã¨ã—ã¦ã€ãã®ã¾ã¾åºƒå‘Šã¨ã—ã¦ä½¿ãˆã‚‹ã‚‚ã®ã«ã—ã¦ãã ã•ã„ã€‚{json_instruction}"""
            }
            
            new_user_message = {
                "role": "user", 
                "content": f"{orientation}\n\n{self_reflection_prompt}"
            }
            
            messages = [system_message] + conversation_messages + [new_user_message]
            
            if use_json_mode:
                response = openai.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=3000,
                    temperature=temperature,
                    response_format={"type": "json_object"}
                )
            else:
                response = openai.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=3000,
                    temperature=temperature
                )
                
            response_text = response.choices[0].message.content
        
        # demo_3ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼šãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãã®ã¾ã«è¿”ã™
        return response_text
            
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

def supports_json_mode(model: str) -> bool:
    """ãƒ¢ãƒ‡ãƒ«ãŒJSON Modeã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    # o1ç³»ã€o3ç³»ã®æ¨è«–ãƒ¢ãƒ‡ãƒ«ã¯JSON Modeã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„
    unsupported_prefixes = ['o1-', 'o3-']
    return not any(prefix in model.lower() for prefix in unsupported_prefixes)

def generate_staged_copy(orientation: str, stage_prompt: str, conversation_messages: List[Dict], model: str = "gpt-4o", temperature: float = 0.9) -> Tuple[str, Dict]:
    """æ®µéšçš„ã‚³ãƒ”ãƒ¼ç”Ÿæˆï¼ˆJSONå‡ºåŠ›å¯¾å¿œï¼‰"""
    openai.api_key = OPENAI_API_KEY
    
    # ãƒ¢ãƒ‡ãƒ«ç¨®åˆ¥ã‚’åˆ¤å®š
    is_o1_pro = "o1-pro" in model.lower()
    is_o3_or_o1_other = any(prefix in model.lower() for prefix in ['o1-', 'o3-']) and not is_o1_pro
    use_json_mode = supports_json_mode(model)
    
    # JSONå‡ºåŠ›ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ‹¡å¼µ
    json_instruction = """

å›ç­”ã¯å¿…ãšJSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã®å½¢å¼ã«å¾“ã£ã¦ãã ã•ã„ï¼š

{
  "copies": [
    "ã‚³ãƒ”ãƒ¼1",
    "ã‚³ãƒ”ãƒ¼2", 
    ...ï¼ˆ20å€‹ï¼‰
  ]
}

what to sayã¯æ€è€ƒã®éç¨‹ã¨ã—ã¦é‡è¦ã§ã™ãŒã€æœ€çµ‚çš„ãªJSONã«ã¯å«ã‚ãšã€ã‚³ãƒ”ãƒ¼ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
JSONä»¥å¤–ã®èª¬æ˜ã‚„å‰ç½®ãã¯ä¸€åˆ‡å«ã‚ãšã€ç´”ç²‹ãªJSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""
    
    try:
        if is_o1_pro:
            # o1-proã¯Responses APIã®ã¿å¯¾å¿œ
            if conversation_messages:
                conversation_text = "\n\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_messages])
                user_message = f"{orientation}\n\néå»ã®ä¼šè©±:\n{conversation_text}\n\n{stage_prompt}{json_instruction}"
            else:
                user_message = f"{orientation}\n\n{stage_prompt}{json_instruction}"
            
            response = openai.responses.create(
                model=model,
                input=user_message,
                reasoning={"effort": "high"}
            )
            response_text = response.choices[0].message.content
            
        elif is_o3_or_o1_other:
            # ãã®ä»–ã®æ¨è«–ãƒ¢ãƒ‡ãƒ«ç”¨ã®å‡¦ç†
            if conversation_messages:
                conversation_text = "\n\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_messages])
                user_message = f"{orientation}\n\néå»ã®ä¼šè©±:\n{conversation_text}\n\n{stage_prompt}{json_instruction}"
            else:
                user_message = f"{orientation}\n\n{stage_prompt}{json_instruction}"
            
            messages = [{"role": "user", "content": user_message}]
            
            response = openai.chat.completions.create(
                model=model,
                messages=messages,
                max_completion_tokens=3000
            )
            response_text = response.choices[0].message.content
        
        else:
            # æ¨™æº–GPTãƒ¢ãƒ‡ãƒ«ç”¨ã®å‡¦ç†
            system_message = {
                "role": "system", 
                "content": f"""ã‚ãªãŸã¯å„ªç§€ãªã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
æ®µéšçš„ã«ã‚³ãƒ”ãƒ¼ã‚’æ”¹å–„ã—ã¦ã„ãã¾ã™ã€‚ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´ã‚’è¸ã¾ãˆã¦ã€æŒ‡ç¤ºã«å¾“ã£ã¦ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆãƒ»æ”¹å–„ã—ã¦ãã ã•ã„ã€‚
é‡è¦ï¼šç”Ÿæˆã™ã‚‹ã‚³ãƒ”ãƒ¼ã¯ç´”ç²‹ãªã‚³ãƒ”ãƒ¼æ–‡è¨€ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚èª¬æ˜æ–‡ã€åˆ†æã€å‹ç•ªå·ã€å‹åãªã©ã®ä½™è¨ˆãªæƒ…å ±ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚ã‚³ãƒ”ãƒ¼ã¯å®Œæˆå“ã¨ã—ã¦ã€ãã®ã¾ã¾åºƒå‘Šã¨ã—ã¦ä½¿ãˆã‚‹ã‚‚ã®ã«ã—ã¦ãã ã•ã„ã€‚{json_instruction}"""
            }
            
            new_user_message = {"role": "user", "content": f"{orientation}\n\n{stage_prompt}"}
            messages = [system_message] + conversation_messages + [new_user_message]
            
            if use_json_mode:
                response = openai.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=3000,
                    temperature=temperature,
                    response_format={"type": "json_object"}
                )
            else:
                response = openai.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=3000,
                    temperature=temperature
                )
                
            response_text = response.choices[0].message.content
        
        # JSONã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦è¡¨ç¤ºç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        parsed_json = parse_json_response(response_text)
        formatted_result = format_copies_display(parsed_json)
        return formatted_result, parsed_json
            
    except Exception as e:
        error_msg = str(e)
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_msg}", {}

def generate_how_to_say_refinement(copies: str, orientation: str, model: str = "gpt-4o", temperature: float = 0.9) -> str:
    """HOW TO SAYå‹ã‚’ä½¿ã£ã¦ã‚³ãƒ”ãƒ¼ã‚’æ´—ç·´ï¼ˆJSONå‡ºåŠ›å¯¾å¿œï¼‰"""
    openai.api_key = OPENAI_API_KEY
    
    # HOW TO SAYå‹ã®è©³ç´°ã‚’æ–‡å­—åˆ—ã§æ§‹ç¯‰
    how_to_say_details = ""
    for i, type_info in enumerate(HOW_TO_SAY_TYPES, 1):
        examples_str = "ã€".join([f'ã€Œ{ex}ã€' for ex in type_info['examples']])
        how_to_say_details += f"{i}ï¼š{type_info['name']}\nä¾‹ï¼š{examples_str}\n\n"
    
    # ãƒ¢ãƒ‡ãƒ«ç¨®åˆ¥ã‚’åˆ¤å®š
    is_o1_pro = "o1-pro" in model.lower()
    is_o3_or_o1_other = any(prefix in model.lower() for prefix in ['o1-', 'o3-']) and not is_o1_pro
    use_json_mode = supports_json_mode(model)
    
    json_instruction = """

å›ç­”ã¯å¿…ãšJSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã®å½¢å¼ã«å¾“ã£ã¦ãã ã•ã„ï¼š

{
  "refinements": [
    {
      "original": "å…ƒã®ã‚³ãƒ”ãƒ¼",
      "type": "å‹Xï¼šå‹å",
      "copy": "æ´—ç·´ã•ã‚ŒãŸã‚³ãƒ”ãƒ¼",
      "reason": "ã“ã®å‹ã‚’é¸ã‚“ã ç†ç”±"
    },
    ...
  ]
}

JSONä»¥å¤–ã®èª¬æ˜ã‚„å‰ç½®ãã¯ä¸€åˆ‡å«ã‚ãšã€ç´”ç²‹ãªJSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""

    prompt = f"""
ä»¥ä¸‹ã®20å€‹ã®ã‚³ãƒ”ãƒ¼ã‚’åˆ†æã—ã€ãã‚Œãã‚Œã«æœ€ã‚‚é©ã—ãŸã€ŒHow to Sayã€ã®å‹ã‚’é¸ã‚“ã§ã€ãã®å‹ã«å½“ã¦ã¯ã‚ã¦æ´—ç·´ã—ã¦ãã ã•ã„ã€‚

ã€ã‚ªãƒªã‚¨ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã€‘
{orientation}

ã€ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ”ãƒ¼ã€‘
{copies}

ã€How to Sayå‹ä¸€è¦§ã€‘
{how_to_say_details}

ã€é‡è¦ãªæŒ‡ç¤ºã€‘
1. å„ã‚³ãƒ”ãƒ¼ã‚’åˆ†æã—ã€æœ€ã‚‚é©ã—ãŸå‹ï¼ˆ1-20ï¼‰ã‚’åˆ¤å®šã—ã¦ãã ã•ã„
2. ãã®å‹ã®ç‰¹å¾´ã‚’æ´»ã‹ã—ã¦ã€ã‚³ãƒ”ãƒ¼ã‚’ã‚ˆã‚ŠåŠ¹æœçš„ã«æ´—ç·´ã—ã¦ãã ã•ã„
3. æ´—ç·´ã•ã‚ŒãŸã‚³ãƒ”ãƒ¼ã¯ç´”ç²‹ãªã‚³ãƒ”ãƒ¼æ–‡è¨€ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„
4. å‹ç•ªå·ã‚„å‹åã€èª¬æ˜ãªã©ã®ä½™è¨ˆãªæƒ…å ±ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„
5. æ´—ç·´ã•ã‚ŒãŸã‚³ãƒ”ãƒ¼ã¯å®Œæˆå“ã¨ã—ã¦ã€ãã®ã¾ã¾åºƒå‘Šã¨ã—ã¦ä½¿ãˆã‚‹ã‚‚ã®ã«ã—ã¦ãã ã•ã„

{json_instruction}
"""
    
    try:
        if is_o1_pro:
            response = openai.responses.create(
                model=model,
                input=prompt,
                reasoning={"effort": "high"}
            )
            response_text = response.choices[0].message.content
            
        elif is_o3_or_o1_other:
            messages = [{"role": "user", "content": prompt}]
            
            response = openai.chat.completions.create(
                model=model,
                messages=messages,
                max_completion_tokens=3000
            )
            response_text = response.choices[0].message.content
        
        else:
            system_message = {
                "role": "system", 
                "content": f"""ã‚ãªãŸã¯å„ªç§€ãªã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
ä¸ãˆã‚‰ã‚ŒãŸã‚³ãƒ”ãƒ¼ã‚’20ç¨®é¡ã®ã€ŒHow to Sayã€å‹ã«å½“ã¦ã¯ã‚ã¦ã€ã‚ˆã‚ŠåŠ¹æœçš„ã«æ´—ç·´ã—ã¦ãã ã•ã„ã€‚
å„å‹ã®ç‰¹å¾´ã‚’ç†è§£ã—ã€ã‚³ãƒ”ãƒ¼ã®æœ¬è³ªã‚’ä¿ã¡ãªãŒã‚‰ã€ã‚ˆã‚Šå°è±¡çš„ã§è¨˜æ†¶ã«æ®‹ã‚‹è¡¨ç¾ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚

é‡è¦ï¼šæ´—ç·´ã•ã‚ŒãŸã‚³ãƒ”ãƒ¼ã¯ç´”ç²‹ãªã‚³ãƒ”ãƒ¼æ–‡è¨€ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚å‹ç•ªå·ã‚„å‹åã€èª¬æ˜ãªã©ã®ä½™è¨ˆãªæƒ…å ±ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚æ´—ç·´ã•ã‚ŒãŸã‚³ãƒ”ãƒ¼ã¯å®Œæˆå“ã¨ã—ã¦ã€ãã®ã¾ã¾åºƒå‘Šã¨ã—ã¦ä½¿ãˆã‚‹ã‚‚ã®ã«ã—ã¦ãã ã•ã„ã€‚{json_instruction}"""
            }
            
            messages = [system_message, {"role": "user", "content": prompt}]
            
            if use_json_mode:
                response = openai.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=3000,
                    temperature=temperature,
                    response_format={"type": "json_object"}
                )
            else:
                response = openai.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=3000,
                    temperature=temperature
                )
            
            response_text = response.choices[0].message.content
        
        # demo_3ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼šãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãã®ã¾ã«è¿”ã™
        return response_text
            
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

def generate_custom_copy(orientation: str, custom_prompt: str, conversation_messages: List[Dict], model: str = "gpt-4o", temperature: float = 0.9) -> Tuple[str, Dict]:
    """ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã‚ˆã‚‹ã‚³ãƒ”ãƒ¼ç”Ÿæˆï¼ˆJSONå‡ºåŠ›å¯¾å¿œï¼‰"""
    openai.api_key = OPENAI_API_KEY
    
    # ãƒ¢ãƒ‡ãƒ«ç¨®åˆ¥ã‚’åˆ¤å®š
    is_o1_pro = "o1-pro" in model.lower()
    is_o3_or_o1_other = any(prefix in model.lower() for prefix in ['o1-', 'o3-']) and not is_o1_pro
    use_json_mode = supports_json_mode(model)
    
    json_instruction = """

å›ç­”ã¯å¿…ãšJSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã®å½¢å¼ã«å¾“ã£ã¦ãã ã•ã„ï¼š

{
  "copies": [
    "æ”¹å–„ã•ã‚ŒãŸã‚³ãƒ”ãƒ¼1",
    "æ”¹å–„ã•ã‚ŒãŸã‚³ãƒ”ãƒ¼2",
    ...ï¼ˆ20å€‹ç¨‹åº¦ï¼‰
  ]
}

JSONä»¥å¤–ã®èª¬æ˜ã‚„å‰ç½®ãã¯ä¸€åˆ‡å«ã‚ãšã€ç´”ç²‹ãªJSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""
    
    try:
        if is_o1_pro:
            if conversation_messages:
                conversation_text = "\n\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_messages])
                user_message = f"{orientation}\n\néå»ã®ä¼šè©±:\n{conversation_text}\n\n{custom_prompt}{json_instruction}"
            else:
                user_message = f"{orientation}\n\n{custom_prompt}{json_instruction}"
            
            response = openai.responses.create(
                model=model,
                input=user_message,
                reasoning={"effort": "high"}
            )
            response_text = response.choices[0].message.content
            
        elif is_o3_or_o1_other:
            if conversation_messages:
                conversation_text = "\n\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_messages])
                user_message = f"{orientation}\n\néå»ã®ä¼šè©±:\n{conversation_text}\n\n{custom_prompt}{json_instruction}"
            else:
                user_message = f"{orientation}\n\n{custom_prompt}{json_instruction}"
            
            messages = [{"role": "user", "content": user_message}]
            
            response = openai.chat.completions.create(
                model=model,
                messages=messages,
                max_completion_tokens=3000
            )
            response_text = response.choices[0].message.content
        
        else:
            system_message = {
                "role": "system", 
                "content": f"""ã‚ãªãŸã¯å„ªç§€ãªã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´ã‚’è¸ã¾ãˆã¦ã€ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«åŸºã¥ã„ã¦ã‚³ãƒ”ãƒ¼ã‚’æ”¹å–„ãƒ»ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

é‡è¦ï¼šç”Ÿæˆã™ã‚‹ã‚³ãƒ”ãƒ¼ã¯ç´”ç²‹ãªã‚³ãƒ”ãƒ¼æ–‡è¨€ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚èª¬æ˜æ–‡ã€åˆ†æã€å‹ç•ªå·ã€å‹åãªã©ã®ä½™è¨ˆãªæƒ…å ±ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚ã‚³ãƒ”ãƒ¼ã¯å®Œæˆå“ã¨ã—ã¦ã€ãã®ã¾ã¾åºƒå‘Šã¨ã—ã¦ä½¿ãˆã‚‹ã‚‚ã®ã«ã—ã¦ãã ã•ã„ã€‚{json_instruction}"""
            }
            
            new_user_message = {
                "role": "user", 
                "content": f"{orientation}\n\n{custom_prompt}"
            }
            
            messages = [system_message] + conversation_messages + [new_user_message]
            
            if use_json_mode:
                response = openai.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=3000,
                    temperature=temperature,
                    response_format={"type": "json_object"}
                )
            else:
                response = openai.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=3000,
                    temperature=temperature
                )
            
            response_text = response.choices[0].message.content
        
        # JSONã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦è¡¨ç¤ºç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        parsed_json = parse_json_response(response_text)
        formatted_result = format_copies_display(parsed_json)
        return formatted_result, parsed_json
            
    except Exception as e:
        error_msg = str(e)
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_msg}", {}

def generate_copy_ideas(orientation: str, csv_file: str = None, num_ideas: int = 5, model: str = "gpt-4o", temperature: float = 0.9) -> str:
    """ã‚ªãƒªã‚¨ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ + ä¼šè©±å±¥æ­´ãƒ™ãƒ¼ã‚¹ã®ã‚³ãƒ”ãƒ¼ç”Ÿæˆï¼ˆdemo_3ã®ã‚·ãƒ³ãƒ—ãƒ«ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰"""
    openai.api_key = OPENAI_API_KEY
    
    # ä¼šè©±å±¥æ­´ã‚’èª­ã¿è¾¼ã¿
    conversation_history = load_conversation_history(csv_file)
    
    # ãƒ¢ãƒ‡ãƒ«ç¨®åˆ¥ã‚’åˆ¤å®š
    is_o1_pro = "o1-pro" in model.lower()
    is_o3_or_o1_other = any(prefix in model.lower() for prefix in ['o1-', 'o3-']) and not is_o1_pro
    
    try:
        if is_o1_pro:
            user_message = f"{orientation}\n\nä¸Šè¨˜ã®æƒ…å ±ã‚’åŸºã«ã€åŠ¹æœçš„ãªã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ã‚’{num_ideas}å€‹ä½œæˆã—ã¦ãã ã•ã„ã€‚"
            
            response = openai.responses.create(
                model=model,
                input=user_message,
                reasoning={"effort": "high"}
            )
            return response.choices[0].message.content
            
        elif is_o3_or_o1_other:
            user_message = f"{orientation}\n\nä¸Šè¨˜ã®æƒ…å ±ã‚’åŸºã«ã€åŠ¹æœçš„ãªã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ã‚’{num_ideas}å€‹ä½œæˆã—ã¦ãã ã•ã„ã€‚"
            
            messages = [{"role": "user", "content": user_message}]
            
            response = openai.chat.completions.create(
                model=model,
                messages=messages,
                max_completion_tokens=1200
            )
            return response.choices[0].message.content
        
        else:
            system_message = {
                "role": "system", 
                "content": f"""ã‚ãªãŸã¯å„ªç§€ãªã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´ã‚’è¸ã¾ãˆã¦ã€ä¸ãˆã‚‰ã‚ŒãŸã‚ªãƒªã‚¨ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’æ´»ç”¨ã—ã€åŠ¹æœçš„ãªã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

å‡ºåŠ›ã¯ç°¡æ½”ã§ã€ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆãŒã‚ã‚Šã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«åˆºã•ã‚‹ã‚‚ã®ã‚’{num_ideas}å€‹å³é¸ã—ã¦ãã ã•ã„ã€‚
ã‚³ãƒ”ãƒ¼ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""
            }
            
            new_user_message = {
                "role": "user", 
                "content": f"{orientation}\n\næœ€çµ‚çš„ã«{num_ideas}å€‹ã®å³é¸ã•ã‚ŒãŸã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
            }
            
            messages = [system_message] + conversation_history + [new_user_message]
                
            response = openai.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=1200,
                temperature=temperature
            )
                
            return response.choices[0].message.content
            
    except Exception as e:
        error_msg = str(e)
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_msg}"

# Streamlit UI
st.set_page_config(
    page_title="ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼ã«ãªã‚ã†",
    page_icon="ğŸ‘¨",
    layout="wide",
    initial_sidebar_state="collapsed"
)

st.title("ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼ã«ãªã‚ã†ğŸ‘¨ v6")

# A1æ˜æœãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š
st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=Noto+Serif+JP:wght@400;700&display=swap');

.copy-display {
    font-family: 'Noto Serif JP', 'A1æ˜æœ', 'ãƒ’ãƒ©ã‚®ãƒæ˜æœ ProN', 'Hiragino Mincho ProN', 'æ¸¸æ˜æœä½“', 'Yu Mincho', YuMincho, 'HGæ˜æœE', 'MS Pæ˜æœ', 'MS PMincho', serif !important;
    font-size: 16px !important;
    line-height: 1.8 !important;
    padding: 10px !important;
    background-color: transparent !important;
    border-radius: 5px !important;
    border: 1px solid rgba(255, 255, 255, 0.3) !important;
    color: #ffffff !important;
    white-space: pre-wrap !important;
}

/* Streamlitç‰¹æœ‰ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ */
.stCheckbox > label > div[data-testid="stMarkdownContainer"] > p {
    font-family: 'Noto Serif JP', 'A1æ˜æœ', 'ãƒ’ãƒ©ã‚®ãƒæ˜æœ ProN', 'Hiragino Mincho ProN', 'æ¸¸æ˜æœä½“', 'Yu Mincho', YuMincho, 'HGæ˜æœE', 'MS Pæ˜æœ', 'MS PMincho', serif !important;
    font-size: 15px !important;
    line-height: 1.7 !important;
}

/* ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹å…¨ä½“ã®ãƒ©ãƒ™ãƒ« */
.stCheckbox label {
    font-family: 'Noto Serif JP', 'A1æ˜æœ', 'ãƒ’ãƒ©ã‚®ãƒæ˜æœ ProN', 'Hiragino Mincho ProN', 'æ¸¸æ˜æœä½“', 'Yu Mincho', YuMincho, 'HGæ˜æœE', 'MS Pæ˜æœ', 'MS PMincho', serif !important;
    font-size: 15px !important;
    line-height: 1.7 !important;
}

/* textareaã®ä¸­èº«ã‚‚æ˜æœã« */
.stTextArea textarea {
    font-family: 'Noto Serif JP', 'A1æ˜æœ', 'ãƒ’ãƒ©ã‚®ãƒæ˜æœ ProN', 'Hiragino Mincho ProN', 'æ¸¸æ˜æœä½“', 'Yu Mincho', YuMincho, 'HGæ˜æœE', 'MS Pæ˜æœ', 'MS PMincho', serif !important;
}

/* ã‚ˆã‚Šå…·ä½“çš„ãªStreamlitã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ */
div[data-testid="stMarkdownContainer"] p {
    font-family: 'Noto Serif JP', 'A1æ˜æœ', 'ãƒ’ãƒ©ã‚®ãƒæ˜æœ ProN', 'Hiragino Mincho ProN', 'æ¸¸æ˜æœä½“', 'Yu Mincho', YuMincho, 'HGæ˜æœE', 'MS Pæ˜æœ', 'MS PMincho', serif !important;
}

/* å…¨ä½“çš„ãªãƒ†ã‚­ã‚¹ãƒˆè¦ç´ ã¸ã®é©ç”¨ */
.element-container div[data-testid="stMarkdownContainer"] {
    font-family: 'Noto Serif JP', 'A1æ˜æœ', 'ãƒ’ãƒ©ã‚®ãƒæ˜æœ ProN', 'Hiragino Mincho ProN', 'æ¸¸æ˜æœä½“', 'Yu Mincho', YuMincho, 'HGæ˜æœE', 'MS Pæ˜æœ', 'MS PMincho', serif !important;
}
</style>
""", unsafe_allow_html=True)

st.markdown("---")

# ãƒ¢ãƒ‡ãƒ«é¸æŠ
selected_model, model_info = display_model_selector()

st.sidebar.markdown("---")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ - Temperatureè¨­å®š
st.sidebar.header("ğŸŒ¡ï¸ å‰µé€ æ€§è¨­å®š")
temperature = st.sidebar.slider(
    "Temperatureï¼ˆå‰µé€ æ€§ãƒ¬ãƒ™ãƒ«ï¼‰",
    min_value=0.0,
    max_value=1.5,
    value=1.2,
    step=0.1
)



st.sidebar.markdown("---")

# ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰é¸æŠ
st.sidebar.header("ğŸ›ï¸ ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰")
generation_mode = st.sidebar.radio(
    "ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠ",
    ["ä¸€æ‹¬ç”Ÿæˆ", "æ®µéšçš„ç”Ÿæˆ"],
    help="ä¸€æ‹¬ç”Ÿæˆï¼š3æ®µéšã‚’é€£ç¶šå®Ÿè¡Œ\næ®µéšçš„ç”Ÿæˆï¼šå„æ®µéšã‚’å€‹åˆ¥å®Ÿè¡Œ"
)

# ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
if generation_mode == "æ®µéšçš„ç”Ÿæˆ":
    # æ®µéšçš„ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
    if 'execution_results' not in st.session_state:
        st.session_state.execution_results = []  # å®Ÿè¡Œé †ã«ãƒ–ãƒ­ãƒƒã‚¯çµæœã‚’æ ¼ç´
    if 'conversation_history' not in st.session_state:
        st.session_state.conversation_history = []
    if 'current_orientation' not in st.session_state:
        st.session_state.current_orientation = ""
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚ªãƒªã‚¨ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
        default_orientation = DEFAULT_ORIENTATION_TEXT
        
        # ã‚ªãƒªã‚¨ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å…¥åŠ›ã‚¨ãƒªã‚¢
        orientation = st.text_area(
            "ä¼æ¥­æƒ…å ±ãƒ»èª²é¡Œãƒ»ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç­‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            value=st.session_state.get('staged_orientation', default_orientation),
            height=300
        )
        
        st.markdown("---")
        
        # å„ãƒ–ãƒ­ãƒƒã‚¯ã‚’ç‹¬ç«‹å®Ÿè¡Œå¯èƒ½ã«è¡¨ç¤º
        for i, block_info in enumerate(COPY_BLOCKS):
            block_id = block_info['id']
            
            # ãƒ–ãƒ­ãƒƒã‚¯è¡¨ç¤º
            with st.container():
                col_btn = st.columns([1])[0]
                
                with col_btn:
                    if st.button(
                        f"â–¶ï¸ {block_info['title']}",
                        key=f"block_{block_id}",
                        type="primary",
                        use_container_width=True
                    ):
                        if not orientation:
                            st.error("ã‚ªãƒªã‚¨ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
                        else:
                            # ã‚ªãƒªã‚¨ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä¿å­˜
                            st.session_state.current_orientation = orientation
                            
                            # é¸æŠã•ã‚ŒãŸã‚³ãƒ”ãƒ¼ãŒã‚ã‚‹å ´åˆã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ 
                            enhanced_prompt = block_info['prompt']
                            if ('unified_selected_copies' in st.session_state and 
                                st.session_state.unified_selected_copies and 
                                'accumulated_copies' in st.session_state):
                                
                                selected_copies = []
                                for idx in st.session_state.unified_selected_copies:
                                    if idx < len(st.session_state.accumulated_copies):
                                        selected_copies.append(st.session_state.accumulated_copies[idx]['copy'])
                                
                                if selected_copies:
                                    selected_copies_text = '\n'.join([f"â€¢ {copy}" for copy in selected_copies])
                                    enhanced_prompt += f"\n\nã€å‚è€ƒã€‘ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç‰¹ã«æ°—ã«å…¥ã£ã¦ã„ãŸã‚³ãƒ”ãƒ¼ï¼š\n{selected_copies_text}\n\nã“ã‚Œã‚‰ã®æ–¹å‘æ€§ã‚„è¡¨ç¾ã‚¹ã‚¿ã‚¤ãƒ«ã‚‚å‚è€ƒã«ã—ãªãŒã‚‰ã€æ–°ã—ã„ã‚³ãƒ”ãƒ¼ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
                            
                            with st.spinner(f"ç”Ÿæˆä¸­... (ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {selected_model})"):
                                result, parsed_json = generate_staged_copy(
                                    orientation, 
                                    enhanced_prompt, 
                                    st.session_state.conversation_history,
                                    selected_model,
                                    temperature
                                )
                                
                            # å®Ÿè¡Œçµæœã‚’é †ç•ªã«è¿½åŠ 
                            execution_result = {
                                'id': block_id,
                                'title': block_info['title'],
                                'prompt': enhanced_prompt,
                                'result': result,
                                'raw_json': parsed_json,
                                'timestamp': time.time()
                            }
                            st.session_state.execution_results.append(execution_result)
                            
                            # ä¼šè©±å±¥æ­´ã«è¿½åŠ ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ï¼‰
                            st.session_state.conversation_history.append({
                                "role": "user", 
                                "content": enhanced_prompt
                            })
                            st.session_state.conversation_history.append({
                                "role": "assistant", 
                                "content": result
                            })
                            
                            st.rerun()
                

                
                st.markdown("---")
        

        
        # ã‚«ã‚¹ã‚¿ãƒ ãƒ–ãƒ­ãƒƒã‚¯
        with st.container():
            col_input_custom = st.columns([1])[0]
            
            with col_input_custom:
                custom_prompt = st.text_area(
                    "ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ»æŒ‡ç¤ºã‚’å…¥åŠ›",
                    height=100,
                    key="custom_prompt_input"
                )
                
                if st.button("ã‚«ã‚¹ã‚¿ãƒ å®Ÿè¡Œ", type="primary", use_container_width=True):
                    if not orientation:
                        st.error("ã‚ªãƒªã‚¨ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
                    elif not custom_prompt:
                        st.error("ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
                    else:
                        # ã‚ªãƒªã‚¨ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä¿å­˜
                        st.session_state.current_orientation = orientation
                        
                        # é¸æŠã•ã‚ŒãŸã‚³ãƒ”ãƒ¼ãŒã‚ã‚‹å ´åˆã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ 
                        enhanced_custom_prompt = custom_prompt
                        if ('unified_selected_copies' in st.session_state and 
                            st.session_state.unified_selected_copies and 
                            'accumulated_copies' in st.session_state):
                            
                            selected_copies = []
                            for idx in st.session_state.unified_selected_copies:
                                if idx < len(st.session_state.accumulated_copies):
                                    selected_copies.append(st.session_state.accumulated_copies[idx]['copy'])
                            
                            if selected_copies:
                                selected_copies_text = '\n'.join([f"â€¢ {copy}" for copy in selected_copies])
                                enhanced_custom_prompt += f"\n\nã€å‚è€ƒã€‘ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç‰¹ã«æ°—ã«å…¥ã£ã¦ã„ãŸã‚³ãƒ”ãƒ¼ï¼š\n{selected_copies_text}\n\nã“ã‚Œã‚‰ã®æ–¹å‘æ€§ã‚„è¡¨ç¾ã‚¹ã‚¿ã‚¤ãƒ«ã‚‚å‚è€ƒã«ã—ãªãŒã‚‰ã€æ–°ã—ã„ã‚³ãƒ”ãƒ¼ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
                        
                        with st.spinner(f"ç”Ÿæˆä¸­... (ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {selected_model})"):
                            result, parsed_json = generate_custom_copy(
                                orientation, 
                                enhanced_custom_prompt, 
                                st.session_state.conversation_history,
                                selected_model,
                                temperature
                            )
                            
                        # ã‚«ã‚¹ã‚¿ãƒ å®Ÿè¡Œçµæœã‚’è¿½åŠ 
                        execution_result = {
                            'id': 'custom',
                            'title': 'ã‚«ã‚¹ã‚¿ãƒ å®Ÿè¡Œ',
                            'prompt': enhanced_custom_prompt,
                            'result': result,
                            'raw_json': parsed_json,
                            'timestamp': time.time()
                        }
                        st.session_state.execution_results.append(execution_result)
                        
                        # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
                        st.session_state.conversation_history.append({
                            "role": "user", 
                            "content": enhanced_custom_prompt
                        })
                        st.session_state.conversation_history.append({
                            "role": "assistant", 
                            "content": result
                        })
                        
                        st.rerun()
                

        
        st.markdown("---")
        
        # ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³
        if len(st.session_state.execution_results) > 0:
            st.markdown("### ãƒªã‚»ãƒƒãƒˆæ©Ÿèƒ½")
            col_reset, col_desc_reset = st.columns([1, 2])
            
            with col_reset:
                if st.button("å…¨å®Ÿè¡Œå±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆ", type="secondary", use_container_width=True):
                    st.session_state.execution_results = []
                    st.session_state.conversation_history = []
                    st.session_state.current_orientation = ""
                    # æ–°æ©Ÿèƒ½ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚‚ã‚¯ãƒªã‚¢
                    if 'unified_selected_copies' in st.session_state:
                        del st.session_state.unified_selected_copies
                    if 'unified_feedback_result' in st.session_state:
                        del st.session_state.unified_feedback_result
                    # è“„ç©ã•ã‚ŒãŸã‚³ãƒ”ãƒ¼ã‚‚ã‚¯ãƒªã‚¢
                    if 'accumulated_copies' in st.session_state:
                        del st.session_state.accumulated_copies
                    st.rerun()
            

    
    with col2:
        
        if st.session_state.execution_results:
            if len(st.session_state.execution_results) > 0:
                st.markdown("---")
                st.markdown("## ã‚³ãƒ”ãƒ¼é¸æŠ")
                
                # ã™ã¹ã¦ã®ã‚³ãƒ”ãƒ¼ã‚’è“„ç©ã™ã‚‹å½¢ã§åé›†
                all_copies = []
                all_source_info = []
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«è“„ç©ã•ã‚ŒãŸã‚³ãƒ”ãƒ¼ã‚’åˆæœŸåŒ–ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
                if 'accumulated_copies' not in st.session_state:
                    st.session_state.accumulated_copies = []
                
                # æœ€æ–°ã®å®Ÿè¡Œçµæœã‹ã‚‰ã‚³ãƒ”ãƒ¼ã‚’å–å¾—
                if st.session_state.execution_results:
                    latest_result = st.session_state.execution_results[-1]
                    raw_json = latest_result.get('raw_json', {})
                    
                    # JSONã‹ã‚‰ã‚³ãƒ”ãƒ¼ãƒªã‚¹ãƒˆã‚’æŠ½å‡º
                    copies_list = extract_copies_list(raw_json)

                    # è“„ç©ã•ã‚ŒãŸã‚³ãƒ”ãƒ¼ãƒªã‚¹ãƒˆã«æœ€æ–°çµæœã®ã‚³ãƒ”ãƒ¼ãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆã®ã¿è¿½åŠ 
                    for copy in copies_list:
                        if copy not in [item['copy'] for item in st.session_state.accumulated_copies]:
                            st.session_state.accumulated_copies.append({
                                'copy': copy,
                                'source': latest_result['title'],
                                'execution_index': len(st.session_state.execution_results) - 1
                            })
                
                # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æçµæœã‹ã‚‰ã‚³ãƒ”ãƒ¼ã‚’è¿½åŠ 
                if 'unified_feedback_result' in st.session_state:
                    feedback_text = st.session_state.unified_feedback_result
                    # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯çµæœã‹ã‚‰ã€Œæ”¹å–„ã•ã‚ŒãŸã‚³ãƒ”ãƒ¼ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
                    if "ã€âœ¨ æ”¹å–„ã•ã‚ŒãŸã‚³ãƒ”ãƒ¼ã€‘" in feedback_text:
                        lines = feedback_text.split("\n")
                        in_copy_section = False
                        feedback_copies = []
                        for line in lines:
                            if "ã€âœ¨ æ”¹å–„ã•ã‚ŒãŸã‚³ãƒ”ãƒ¼ã€‘" in line:
                                in_copy_section = True
                                continue
                            elif line.startswith("ã€") and in_copy_section:
                                break
                            elif in_copy_section and line.strip() and not line.startswith("ã€"):
                                # ç•ªå·ä»˜ãã®è¡Œã‚’æŠ½å‡º
                                if ". " in line and any(char.isdigit() for char in line.split(". ")[0]):
                                    copy_text = ". ".join(line.split(". ")[1:]).strip()
                                    if copy_text:
                                        feedback_copies.append(copy_text)
                        
                        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚³ãƒ”ãƒ¼ã‚’è“„ç©ãƒªã‚¹ãƒˆã«è¿½åŠ ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼‰
                        for copy in feedback_copies:
                            if copy not in [item['copy'] for item in st.session_state.accumulated_copies]:
                                st.session_state.accumulated_copies.append({
                                    'copy': copy,
                                    'source': "ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æ",
                                    'stage_num': 999  # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç”¨ã®ç‰¹åˆ¥ãªç•ªå·
                                })
                
                # è“„ç©ã•ã‚ŒãŸã‚³ãƒ”ãƒ¼ã‚’all_copiesã«è¿½åŠ ï¼ˆæœ€æ–°20å€‹ã®ã¿è¡¨ç¤ºï¼‰
                for item in st.session_state.accumulated_copies[-20:]:  # æœ€æ–°20å€‹ã®ã¿
                    all_copies.append(item['copy'])
                    all_source_info.append(item['source'])
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
                if 'unified_selected_copies' not in st.session_state:
                    st.session_state.unified_selected_copies = []
                
                # ã‚³ãƒ”ãƒ¼é¸æŠUI
                if all_copies:
                    selected_indices = []
                    
                    for i, copy in enumerate(all_copies):
                        if st.checkbox(
                            f"{copy}",
                            key=f"unified_checkbox_{i}",
                            value=i in st.session_state.unified_selected_copies
                        ):
                            selected_indices.append(i)
                    
                    # é¸æŠçŠ¶æ…‹ã‚’æ›´æ–°
                    st.session_state.unified_selected_copies = selected_indices
                    
                    # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æãƒœã‚¿ãƒ³
                    if len(selected_indices) > 0 and len(selected_indices) < len(all_copies):
                        good_copies = [all_copies[i] for i in selected_indices]
                        bad_copies = [all_copies[i] for i in range(len(all_copies)) if i not in selected_indices]
                        
                        if st.button(
                            "æ”¹å–„ã‚³ãƒ”ãƒ¼ã‚’ç”Ÿæˆ",
                            key="unified_analyze_feedback",
                            type="primary"
                        ):
                            with st.spinner("ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æä¸­..."):
                                feedback_result = generate_feedback_based_copy(
                                    st.session_state.current_orientation,
                                    good_copies,
                                    bad_copies,
                                    st.session_state.conversation_history,
                                    selected_model,
                                    temperature
                                )
                                
                                # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯çµæœã‚’ä¿å­˜
                                st.session_state.unified_feedback_result = feedback_result
                            
                            st.success("å®Œäº†ã—ã¾ã—ãŸ")
                            st.rerun()
                    

                
                if 'unified_feedback_result' in st.session_state:
                    st.download_button(
                        label="åˆ†æçµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=st.session_state.unified_feedback_result,
                        file_name="feedback_analysis_detailed.txt",
                        mime="text/plain",
                        key="download_unified_feedback"
                    )
                
                st.markdown("---")
                
                # å…¨å®Ÿè¡Œçµæœã®çµ±åˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                all_results = ""
                for i, execution in enumerate(st.session_state.execution_results):
                    all_results += f"=== å®Ÿè¡Œ{i+1}: {execution['title']} ===\n"
                    all_results += f"å®Ÿè¡Œæ™‚åˆ»: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(execution['timestamp']))}\n"
                    all_results += f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {execution['prompt'][:100]}...\n\n"
                    all_results += f"{execution['result']}\n\n"
                    all_results += "=" * 50 + "\n\n"
                
                st.download_button(
                    label="å…¨å®Ÿè¡Œçµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=all_results,
                    file_name="copy_all_executions.txt",
                    mime="text/plain",
                    type="primary"
                )
                
                
        
        # å®Ÿè¡Œå±¥æ­´ã®è¡¨ç¤º
        if st.session_state.execution_results:
            st.markdown("---")
            st.markdown("## å®Ÿè¡Œå±¥æ­´")
            
            # æœ€æ–°ã®å®Ÿè¡Œçµæœã‚’è¡¨ç¤º
            latest_execution = st.session_state.execution_results[-1]
            st.markdown(f"### æœ€æ–°å®Ÿè¡Œ: {latest_execution['title']}")
            st.markdown(f'<div class="copy-display">{latest_execution["result"]}</div>', unsafe_allow_html=True)
            
            # ã€Œã‚‚ã†ä¸€å£°ã€ãƒœã‚¿ãƒ³ã§æœ€æ–°çµæœã‚’ã•ã‚‰ã«ãƒ–ãƒ©ãƒƒã‚·ãƒ¥ã‚¢ãƒƒãƒ—
            if st.button("ğŸ’¡ ã‚‚ã†ä¸€å£°", key="refine_latest_execution", type="primary"):
                # å‚ç…§ã™ã‚‹ã‚ªãƒªã‚¨ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ±ºå®š
                orientation_for_refine = st.session_state.get('current_orientation', orientation)
                base_copies_text = latest_execution["result"]
                with st.spinner("ã•ã‚‰ã«ã‚³ãƒ”ãƒ¼ã‚’ç£¨ã„ã¦ã„ã¾ã™..."):
                    stage2_prompt = "ã©ã‚Œã‚‚åºƒå‘Šçš„ã§å¿ƒãŒå‹•ã‹ãªã„ã€ã‚‚ã£ã¨å¼·ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå¿…è¦ã€‚ä½¿ã„å¤ã•ã‚ŒãŸè¨€ã„å›ã—ã‚’ä½¿ã‚ãšã«ã€å®šå‹çš„ãªæ§‹æ–‡ã¯é¿ã‘ã¦ã€‚äºŒåå€‹ã®ã‚³ãƒ”ãƒ¼ã‚’è€ƒãˆã¦"
                    formatted_refine, parsed_json_refine = generate_staged_copy(
                        orientation_for_refine,
                        stage2_prompt,
                        st.session_state.conversation_history,
                        selected_model,
                        temperature
                    )
                # å®Ÿè¡Œå±¥æ­´ã¸è¿½åŠ 
                refine_execution = {
                    'id': f"{latest_execution['id']}_refined",
                    'title': f"{latest_execution['title']} - ã‚‚ã†ä¸€å£°",
                    'prompt': "Refine latest copies using How to Say self-reflection",
                    'result': formatted_refine,
                    'raw_json': parsed_json_refine,
                    'timestamp': time.time()
                }
                st.session_state.execution_results.append(refine_execution)
                # ä¼šè©±å±¥æ­´ã¸è¿½åŠ 
                st.session_state.conversation_history.append({
                    "role": "user",
                    "content": "ã‚‚ã†ä¸€å£°: æœ€æ–°ã‚³ãƒ”ãƒ¼ã‚’ã•ã‚‰ã«ç£¨ã„ã¦"
                })
                st.session_state.conversation_history.append({
                    "role": "assistant",
                    "content": formatted_refine
                })
                st.rerun()
            
            # å®Ÿè¡Œå±¥æ­´ã‚’ãƒªã‚¹ãƒˆã§è¡¨ç¤º
            if len(st.session_state.execution_results) > 1:
                st.markdown("### å®Ÿè¡Œå±¥æ­´ä¸€è¦§")
                for i, execution in enumerate(reversed(st.session_state.execution_results)):
                    with st.expander(f"å®Ÿè¡Œ{len(st.session_state.execution_results) - i}: {execution['title']} ({time.strftime('%H:%M:%S', time.localtime(execution['timestamp']))})"):
                        st.markdown(f'<div class="copy-display">{execution["result"]}</div>', unsafe_allow_html=True)
                        st.download_button(
                            label=f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ - {execution['title']}",
                            data=execution['result'],
                            file_name=f"{execution['id']}_result.txt",
                            mime="text/plain",
                            key=f"download_{execution['timestamp']}"
                        )

else:
    # ä¸€æ‹¬ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰ï¼ˆ3æ®µéšé€£ç¶šå®Ÿè¡Œï¼‰
    col1, col2 = st.columns([1, 1])
    
    with col1:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚ªãƒªã‚¨ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
        default_orientation_simple = DEFAULT_ORIENTATION_TEXT
        
        # ã‚ªãƒªã‚¨ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å…¥åŠ›ã‚¨ãƒªã‚¢
        orientation = st.text_area(
            "ä¼æ¥­æƒ…å ±ãƒ»èª²é¡Œãƒ»ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç­‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            value=default_orientation_simple,
            height=300
        )
        
        generate_button = st.button("æ›¸ã„ã¦ã¿ã¦", type="primary", use_container_width=True)

    with col2:
        # çµæœè¡¨ç¤ºç”¨ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼ˆå¸¸ã«æœ€æ–°ã‚’è¡¨ç¤ºï¼‰
        result_placeholder = st.empty()
        
        # å‰å›ç”Ÿæˆæ¸ˆã¿ã®çµæœãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤º
        if st.session_state.get('last_batch_result'):
            result_placeholder.markdown(f'<div class="copy-display">{st.session_state.last_batch_result}</div>', unsafe_allow_html=True)

        if generate_button:
            if not orientation:
                st.error("ã‚ªãƒªã‚¨ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            else:
                # ä¼šè©±å±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆ
                st.session_state.batch_conversation_history = []
                st.session_state.batch_results = []
                
                # 3æ®µéšã‚’é€£ç¶šå®Ÿè¡Œ
                final_result = ""  # æœ€çµ‚çµæœã®ã¿è¡¨ç¤ºã™ã‚‹ãŸã‚ã«ä¿æŒ
                for i, stage_info in enumerate(STAGED_PROMPTS, 1):
                    with st.spinner(f"ç”Ÿæˆä¸­... (ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {selected_model})"):
                        result, parsed_json = generate_staged_copy(
                            orientation, 
                            stage_info['prompt'], 
                            st.session_state.batch_conversation_history,
                            selected_model,
                            temperature
                        )
                        # æœ€çµ‚çµæœã‚’ä¿æŒï¼ˆå¾Œã§ã¾ã¨ã‚ã¦è¡¨ç¤ºï¼‰
                        final_result = result
                        
                        # ä¼šè©±å±¥æ­´ã¨çµæœã¯å¿…è¦ã«å¿œã˜ã¦ä¿æŒ
                        st.session_state.batch_results.append({
                            'stage': i,
                            'title': stage_info['title'],
                            'prompt': stage_info['prompt'],
                            'result': result,
                            'raw_json': parsed_json
                        })
                        st.session_state.batch_conversation_history.append({
                            "role": "user", 
                            "content": stage_info['prompt']
                        })
                        st.session_state.batch_conversation_history.append({
                            "role": "assistant", 
                            "content": result
                        })
                
                # ãƒ«ãƒ¼ãƒ—å®Œäº†å¾Œã«æœ€çµ‚çµæœã®ã¿ã‚’è¡¨ç¤º
                if final_result:
                    result_placeholder.markdown(f'<div class="copy-display">{final_result}</div>', unsafe_allow_html=True)
                    # æœ€æ–°çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿æŒ
                    st.session_state.last_batch_result = final_result
 
                # åˆå›ã‚„æœªç”Ÿæˆæ™‚ã®æ¡ˆå†…
                if not st.session_state.get('batch_results'):
                    st.info("ã€Œæ›¸ã„ã¦ã¿ã¦ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ã‚³ãƒ”ãƒ¼ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„")

        # --- ä¸€æ‹¬ç”Ÿæˆå¾Œã®ã‚‚ã†ä¸€å£° ---
        if st.session_state.get('batch_results'):
            latest_batch = st.session_state.batch_results[-1]
            st.markdown("---")
            if st.button("ğŸ’¡ ã‚‚ã†ä¸€å£°", key="batch_refine", type="primary"):
                orientation_for_refine = orientation  # batch mode orientation is local
                stage2_prompt = "ã©ã‚Œã‚‚åºƒå‘Šçš„ã§å¿ƒãŒå‹•ã‹ãªã„ã€ã‚‚ã£ã¨å¼·ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå¿…è¦ã€‚ä½¿ã„å¤ã•ã‚ŒãŸè¨€ã„å›ã—ã‚’ä½¿ã‚ãšã«ã€å®šå‹çš„ãªæ§‹æ–‡ã¯é¿ã‘ã¦ã€‚äºŒåå€‹ã®ã‚³ãƒ”ãƒ¼ã‚’è€ƒãˆã¦"
                with st.spinner("ã•ã‚‰ã«ã‚³ãƒ”ãƒ¼ã‚’ç£¨ã„ã¦ã„ã¾ã™..."):
                    formatted_refine, parsed_json_refine = generate_staged_copy(
                        orientation_for_refine,
                        stage2_prompt,
                        st.session_state.batch_conversation_history,
                        selected_model,
                        temperature
                    )
                    result_placeholder.markdown(f'<div class="copy-display">{formatted_refine}</div>', unsafe_allow_html=True)
                    st.session_state.last_batch_result = formatted_refine

                # å±¥æ­´æ›´æ–°
                refine_entry = {
                    'stage': latest_batch['stage'],
                    'title': f"{latest_batch['title']} - ã‚‚ã†ä¸€å£°",
                    'prompt': stage2_prompt,
                    'result': formatted_refine,
                    'raw_json': parsed_json_refine
                }
                st.session_state.batch_results.append(refine_entry)
                # ä¼šè©±å±¥æ­´
                st.session_state.batch_conversation_history.append({
                    "role": "user",
                    "content": stage2_prompt
                })
                st.session_state.batch_conversation_history.append({
                    "role": "assistant",
                    "content": formatted_refine
                })
                st.rerun()

st.sidebar.markdown("---")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ - ç®¡ç†è€…ç”¨æ©Ÿèƒ½ã®ã¿
st.sidebar.header("ğŸ”§ ç®¡ç†è€…æ©Ÿèƒ½")
if st.sidebar.checkbox("ç®¡ç†è€…ãƒ¢ãƒ¼ãƒ‰", value=False):
    st.sidebar.markdown("---")
    st.sidebar.subheader("ç®¡ç†è€…è¨­å®š")
    
    available_models = get_available_models()
    
    if st.sidebar.button("åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«ä¸€è¦§è¡¨ç¤º"):
        st.sidebar.write("ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«:")
        for model in available_models:
            st.sidebar.write(f"â€¢ {model}")
    
    if st.sidebar.button("ãƒ¢ãƒ‡ãƒ«è©³ç´°æƒ…å ±è¡¨ç¤º"):
        model_categories = get_model_categories()
        for category, models in model_categories.items():
            st.sidebar.write(f"**{category}**")
            for model_id, model_data in models.items():
                if model_id in available_models:
                    st.sidebar.write(f"â€¢ {model_id}: {model_data['price']}")
    
    if st.sidebar.button("å®Ÿè¡ŒçŠ¶æ…‹è¡¨ç¤º"):
        if generation_mode == "æ®µéšçš„ç”Ÿæˆ":
            st.sidebar.json({
                "available_blocks": len(COPY_BLOCKS),
                "completed_executions": len(st.session_state.get('execution_results', [])),
                "conversation_length": len(st.session_state.get('conversation_history', [])),
                "independent_execution": True,
                "selected_model": selected_model
            })
        else:
            st.sidebar.info("æ®µéšçš„ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
        
    # APIè¨­å®šç¢ºèª
    if OPENAI_API_KEY == "sk-proj-your-api-key-here":
        st.sidebar.warning("âš ï¸ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆAPIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™")
    else:
        st.sidebar.success("âœ… APIã‚­ãƒ¼ãŒè¨­å®šæ¸ˆã¿") 