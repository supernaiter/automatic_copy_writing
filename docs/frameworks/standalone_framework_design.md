# ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯è¨­è¨ˆæ›¸

## 1. è¨­è¨ˆæ€æƒ³ï¼šComplete Standalone

### 1.1 å®Ÿè¡Œæ–¹å¼

```bash
# å„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãŒå®Œå…¨ç‹¬ç«‹ã§å‹•ä½œ
python framework_01_brand_key.py
python framework_02_cep_distribution.py
python framework_05_share_of_search.py
python framework_13_meme_emoji.py

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŒ‡å®šã‚‚å¯èƒ½
python framework_05_share_of_search.py --config config.json
python framework_02_cep_distribution.py --brand "ã‚³ã‚«ã‚³ãƒ¼ãƒ©" --category "é£²æ–™"
```

### 1.2 å®Œå…¨ç‹¬ç«‹ã®åŸå‰‡

**âœ… ä¾å­˜é–¢ä¿‚ã‚¼ãƒ­**
- ä»–ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¾å­˜ã—ãªã„
- å…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒªãªã—ï¼ˆå„ãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…è¦ãªé–¢æ•°ã‚’å†…åŒ…ï¼‰
- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚„APIã‚­ãƒ¼è¨­å®šã‚‚ãƒ•ã‚¡ã‚¤ãƒ«å†…å®Œçµ

**âœ… å³åº§å®Ÿè¡Œå¯èƒ½**
- `python framework_XX.py`ã§ã„ããªã‚Šå‹•ã
- è¨­å®šã‚„åˆæœŸåŒ–å‡¦ç†ã‚‚ã‚¹ã‚¯ãƒªãƒ—ãƒˆå†…ã§å®Œçµ
- ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿å†…è”µã§ãƒ‡ãƒ¢å‹•ä½œå¯èƒ½

**âœ… çµæœãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›**
- å®Ÿè¡Œçµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«è‡ªå‹•ä¿å­˜
- CSVã€ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ç”Ÿæˆ
- ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã«ã‚‚è¦ç´„çµæœã‚’è¡¨ç¤º

## 2. ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³å®Ÿè£…ä¾‹

### 2.1 ãƒ•ãƒ¬ãƒ¼ãƒ 5: Share of Search (framework_05_share_of_search.py)

```python
#!/usr/bin/env python3
"""
ãƒ•ãƒ¬ãƒ¼ãƒ 5: Share of Search åˆ†æ
å®Ÿè¡Œæ–¹æ³•: python framework_05_share_of_search.py
"""

import asyncio
import json
import time
import argparse
from datetime import datetime
from typing import List, Dict, Any
import pandas as pd
import matplotlib.pyplot as plt
from pytrends.request import TrendReq
import os

class ShareOfSearchFramework:
    """Share of Search åˆ†æãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ - ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ç‰ˆ"""
    
    def __init__(self):
        self.framework_id = 5
        self.framework_name = "Share of Search"
        self.version = "1.0.0"
        self.output_dir = f"output_framework_05_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(self.output_dir, exist_ok=True)
        
        print(f"ğŸš€ {self.framework_name} v{self.version} é–‹å§‹")
        print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
    
    async def collect_data(self, brand_name: str, competitors: List[str], 
                          category: str, time_range: str = "today 12-m") -> Dict[str, Any]:
        """Share of Search ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»åˆ†æ"""
        
        start_time = time.time()
        print(f"\nğŸ“Š åˆ†æé–‹å§‹: {brand_name} vs {competitors}")
        
        try:
            # Step 1: Google Trends APIæ¥ç¶š
            print("ğŸ”— Google Trends APIæ¥ç¶šä¸­...")
            pytrends = TrendReq(hl='ja-JP', tz=360, timeout=(10, 25))
            
            # Step 2: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ãƒ‡ãƒ¼ã‚¿å–å¾—
            all_keywords = [brand_name] + competitors
            print(f"ğŸ” æ¤œç´¢ãƒ‡ãƒ¼ã‚¿å–å¾—: {all_keywords}")
            
            pytrends.build_payload(all_keywords, timeframe=time_range, geo='JP')
            interest_data = pytrends.interest_over_time()
            
            if interest_data.empty:
                raise Exception("æ¤œç´¢ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            
            # Step 3: Share of Search è¨ˆç®—
            print("ğŸ§® Share of Search è¨ˆç®—ä¸­...")
            sos_data = self._calculate_share_of_search(interest_data, all_keywords)
            
            # Step 4: ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
            trend_analysis = self._analyze_trends(interest_data, all_keywords)
            
            # Step 5: ç«¶åˆãƒã‚¸ã‚·ãƒ§ãƒ³åˆ†æ
            competitive_analysis = self._analyze_competitive_position(sos_data, brand_name)
            
            execution_time = time.time() - start_time
            
            result = {
                "framework_info": {
                    "id": self.framework_id,
                    "name": self.framework_name,
                    "version": self.version,
                    "execution_time": execution_time,
                    "timestamp": datetime.now().isoformat()
                },
                "input_parameters": {
                    "brand_name": brand_name,
                    "competitors": competitors,
                    "category": category,
                    "time_range": time_range
                },
                "share_of_search": sos_data,
                "trend_analysis": trend_analysis,
                "competitive_analysis": competitive_analysis,
                "insights": self._generate_insights(sos_data, trend_analysis, competitive_analysis),
                "raw_data": interest_data.to_dict(),
                "success": True
            }
            
            print(f"âœ… åˆ†æå®Œäº† ({execution_time:.2f}ç§’)")
            return result
            
        except Exception as e:
            error_result = {
                "framework_info": {
                    "id": self.framework_id,
                    "name": self.framework_name,
                    "error": str(e)
                },
                "success": False,
                "execution_time": time.time() - start_time
            }
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return error_result
    
    def _calculate_share_of_search(self, data: pd.DataFrame, keywords: List[str]) -> Dict[str, float]:
        """æ¤œç´¢ã‚·ã‚§ã‚¢è¨ˆç®—"""
        # NaNå€¤ã‚’0ã§ç½®æ›
        data_clean = data[keywords].fillna(0)
        
        # æœŸé–“å¹³å‡ã§ã®å„ãƒ–ãƒ©ãƒ³ãƒ‰ã‚·ã‚§ã‚¢
        total_searches = data_clean.sum(axis=1)
        share_data = {}
        
        for keyword in keywords:
            if total_searches.sum() > 0:
                share_data[keyword] = (data_clean[keyword].sum() / data_clean.sum().sum() * 100)
            else:
                share_data[keyword] = 0.0
        
        return share_data
    
    def _analyze_trends(self, data: pd.DataFrame, keywords: List[str]) -> Dict[str, Any]:
        """ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"""
        trends = {}
        
        for keyword in keywords:
            keyword_data = data[keyword].fillna(0)
            if len(keyword_data) > 1:
                # ç·šå½¢å›å¸°ã§å‚¾å‘è¨ˆç®—
                x = range(len(keyword_data))
                correlation = pd.Series(x).corr(keyword_data)
                
                trend_direction = "increasing" if correlation > 0.1 else "decreasing" if correlation < -0.1 else "stable"
                trend_strength = abs(correlation)
                
                trends[keyword] = {
                    "direction": trend_direction,
                    "strength": trend_strength,
                    "recent_peak": keyword_data.max(),
                    "recent_avg": keyword_data.mean()
                }
        
        return trends
    
    def _analyze_competitive_position(self, sos_data: Dict[str, float], brand_name: str) -> Dict[str, Any]:
        """ç«¶åˆãƒã‚¸ã‚·ãƒ§ãƒ³åˆ†æ"""
        sorted_brands = sorted(sos_data.items(), key=lambda x: x[1], reverse=True)
        brand_rank = next((i+1 for i, (brand, _) in enumerate(sorted_brands) if brand == brand_name), None)
        
        market_leader = sorted_brands[0][0] if sorted_brands else None
        brand_share = sos_data.get(brand_name, 0)
        leader_share = sorted_brands[0][1] if sorted_brands else 0
        
        gap_to_leader = leader_share - brand_share if market_leader != brand_name else 0
        
        return {
            "brand_rank": brand_rank,
            "total_brands": len(sos_data),
            "market_leader": market_leader,
            "brand_share": brand_share,
            "leader_share": leader_share,
            "gap_to_leader": gap_to_leader,
            "competitive_intensity": len([s for s in sos_data.values() if s > 10])  # 10%ä»¥ä¸Šã®ãƒ–ãƒ©ãƒ³ãƒ‰æ•°
        }
    
    def _generate_insights(self, sos_data: Dict[str, float], 
                          trend_analysis: Dict[str, Any], 
                          competitive_analysis: Dict[str, Any]) -> List[str]:
        """æ´å¯Ÿç”Ÿæˆ"""
        insights = []
        
        # æ¤œç´¢ã‚·ã‚§ã‚¢æ´å¯Ÿ
        top_brand = max(sos_data.items(), key=lambda x: x[1])
        insights.append(f"æ¤œç´¢ã‚·ã‚§ã‚¢1ä½ã¯{top_brand[0]}ã§{top_brand[1]:.1f}%")
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰æ´å¯Ÿ
        increasing_brands = [brand for brand, data in trend_analysis.items() 
                           if data["direction"] == "increasing" and data["strength"] > 0.3]
        if increasing_brands:
            insights.append(f"ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰: {', '.join(increasing_brands)}")
        
        # ç«¶åˆçŠ¶æ³æ´å¯Ÿ
        if competitive_analysis["competitive_intensity"] > 3:
            insights.append("æ¿€æˆ¦ã‚«ãƒ†ã‚´ãƒªï¼ˆ10%ä»¥ä¸Šã‚·ã‚§ã‚¢ãƒ–ãƒ©ãƒ³ãƒ‰å¤šæ•°ï¼‰")
        
        # ã‚®ãƒ£ãƒƒãƒ—æ´å¯Ÿ
        if competitive_analysis["gap_to_leader"] > 20:
            insights.append(f"ãƒªãƒ¼ãƒ€ãƒ¼ã¨ã®å·®ãŒå¤§ãã„ï¼ˆ{competitive_analysis['gap_to_leader']:.1f}%å·®ï¼‰")
        
        return insights
    
    def save_results(self, result: Dict[str, Any]) -> None:
        """çµæœä¿å­˜"""
        # JSONä¿å­˜
        json_path = os.path.join(self.output_dir, "sos_analysis_result.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ çµæœä¿å­˜: {json_path}")
        
        # CSVä¿å­˜
        if result.get("success") and "share_of_search" in result:
            sos_df = pd.DataFrame(list(result["share_of_search"].items()), 
                                columns=["Brand", "Share_of_Search_%"])
            csv_path = os.path.join(self.output_dir, "share_of_search.csv")
            sos_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"ğŸ“Š CSVä¿å­˜: {csv_path}")
        
        # å¯è¦–åŒ–ã‚°ãƒ©ãƒ•ä¿å­˜
        if result.get("success"):
            self._save_visualization(result)
    
    def _save_visualization(self, result: Dict[str, Any]) -> None:
        """å¯è¦–åŒ–ã‚°ãƒ©ãƒ•ç”Ÿæˆãƒ»ä¿å­˜"""
        try:
            plt.style.use('default')
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # Share of Search å††ã‚°ãƒ©ãƒ•
            sos_data = result["share_of_search"]
            ax1.pie(sos_data.values(), labels=sos_data.keys(), autopct='%1.1f%%')
            ax1.set_title("Share of Search Distribution")
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘æ£’ã‚°ãƒ©ãƒ•  
            trend_data = result["trend_analysis"]
            brands = list(trend_data.keys())
            strengths = [data["strength"] for data in trend_data.values()]
            colors = ['green' if trend_data[brand]["direction"] == "increasing" 
                     else 'red' if trend_data[brand]["direction"] == "decreasing" 
                     else 'gray' for brand in brands]
            
            ax2.bar(brands, strengths, color=colors)
            ax2.set_title("Trend Strength by Brand")
            ax2.set_ylabel("Trend Strength")
            ax2.tick_params(axis='x', rotation=45)
            
            plt.tight_layout()
            
            chart_path = os.path.join(self.output_dir, "sos_analysis_chart.png")
            plt.savefig(chart_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"ğŸ“ˆ ã‚°ãƒ©ãƒ•ä¿å­˜: {chart_path}")
            
        except Exception as e:
            print(f"âš ï¸  ã‚°ãƒ©ãƒ•ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def print_summary(self, result: Dict[str, Any]) -> None:
        """çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        if not result.get("success"):
            print(f"\nâŒ å®Ÿè¡Œå¤±æ•—: {result.get('framework_info', {}).get('error', 'Unknown error')}")
            return
        
        print(f"\nğŸ“‹ {self.framework_name} å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼")
        print("=" * 50)
        
        # åŸºæœ¬æƒ…å ±
        print(f"ğŸ¯ å¯¾è±¡ãƒ–ãƒ©ãƒ³ãƒ‰: {result['input_parameters']['brand_name']}")
        print(f"ğŸ†š ç«¶åˆãƒ–ãƒ©ãƒ³ãƒ‰: {', '.join(result['input_parameters']['competitors'])}")
        print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {result['framework_info']['execution_time']:.2f}ç§’")
        
        # Share of Searchçµæœ
        print(f"\nğŸ“Š Share of Searchçµæœ:")
        for brand, share in result["share_of_search"].items():
            print(f"  {brand}: {share:.1f}%")
        
        # ç«¶åˆåˆ†æçµæœ
        comp_analysis = result["competitive_analysis"]
        print(f"\nğŸ† ç«¶åˆãƒã‚¸ã‚·ãƒ§ãƒ³:")
        print(f"  é †ä½: {comp_analysis['brand_rank']}/{comp_analysis['total_brands']}ä½")
        print(f"  å¸‚å ´ãƒªãƒ¼ãƒ€ãƒ¼: {comp_analysis['market_leader']}")
        print(f"  ãƒªãƒ¼ãƒ€ãƒ¼ã¨ã®å·®: {comp_analysis['gap_to_leader']:.1f}%")
        
        # æ´å¯Ÿ
        print(f"\nğŸ’¡ ä¸»è¦æ´å¯Ÿ:")
        for insight in result["insights"]:
            print(f"  â€¢ {insight}")
        
        print(f"\nğŸ“ è©³ç´°çµæœã¯ {self.output_dir} ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
SAMPLE_DATA = {
    "brand_name": "ã‚³ã‚«ã‚³ãƒ¼ãƒ©",
    "competitors": ["ãƒšãƒ—ã‚·", "ä¼Šå³è¡›é–€", "åˆå¾Œã®ç´…èŒ¶"],
    "category": "é£²æ–™",
    "time_range": "today 12-m"
}

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="Share of Search åˆ†æãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯")
    parser.add_argument("--brand", default=SAMPLE_DATA["brand_name"], help="å¯¾è±¡ãƒ–ãƒ©ãƒ³ãƒ‰å")
    parser.add_argument("--competitors", nargs='+', default=SAMPLE_DATA["competitors"], help="ç«¶åˆãƒ–ãƒ©ãƒ³ãƒ‰")
    parser.add_argument("--category", default=SAMPLE_DATA["category"], help="ã‚«ãƒ†ã‚´ãƒª")
    parser.add_argument("--timerange", default=SAMPLE_DATA["time_range"], help="åˆ†ææœŸé–“")
    parser.add_argument("--config", help="è¨­å®šJSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")
    
    args = parser.parse_args()
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°èª­ã¿è¾¼ã¿
    if args.config and os.path.exists(args.config):
        with open(args.config, 'r', encoding='utf-8') as f:
            config = json.load(f)
        brand_name = config.get("brand_name", args.brand)
        competitors = config.get("competitors", args.competitors)
        category = config.get("category", args.category)
        time_range = config.get("time_range", args.timerange)
    else:
        brand_name = args.brand
        competitors = args.competitors
        category = args.category  
        time_range = args.timerange
    
    # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å®Ÿè¡Œ
    framework = ShareOfSearchFramework()
    result = await framework.collect_data(brand_name, competitors, category, time_range)
    
    # çµæœä¿å­˜ãƒ»è¡¨ç¤º
    framework.save_results(result)
    framework.print_summary(result)

if __name__ == "__main__":
    asyncio.run(main())
```

### 2.2 ãƒ•ãƒ¬ãƒ¼ãƒ 2: CEPåˆ†å¸ƒ (framework_02_cep_distribution.py)

```python
#!/usr/bin/env python3
"""
ãƒ•ãƒ¬ãƒ¼ãƒ 2: Category Entry Points åˆ†å¸ƒåˆ†æ
å®Ÿè¡Œæ–¹æ³•: python framework_02_cep_distribution.py
"""

import asyncio
import json
import time
import argparse
from datetime import datetime
from typing import List, Dict, Any
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tweepy
import openai
import os
from collections import Counter
import re

class CategoryEntryPointsFramework:
    """Category Entry Points åˆ†æãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ - ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ç‰ˆ"""
    
    def __init__(self):
        self.framework_id = 2
        self.framework_name = "Category Entry Points Distribution"
        self.version = "1.0.0"
        self.output_dir = f"output_framework_02_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(self.output_dir, exist_ok=True)
        
        # APIè¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.twitter_bearer_token = os.getenv("TWITTER_BEARER_TOKEN")
        
        print(f"ğŸš€ {self.framework_name} v{self.version} é–‹å§‹")
        print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
    
    async def collect_data(self, category: str, time_range: str = "30d") -> Dict[str, Any]:
        """Category Entry Points ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»åˆ†æ"""
        
        start_time = time.time()
        print(f"\nğŸ“Š åˆ†æé–‹å§‹: ã‚«ãƒ†ã‚´ãƒª '{category}'")
        
        try:
            # Step 1: SNSæŠ•ç¨¿åé›†
            print("ğŸ¦ SNSæŠ•ç¨¿åé›†ä¸­...")
            social_posts = await self._collect_social_posts(category, time_range)
            
            # Step 2: æ¤œç´¢ã‚¯ã‚¨ãƒªåé›† (æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿)
            print("ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒªåé›†ä¸­...")
            search_queries = await self._collect_search_queries(category)
            
            # Step 3: Entry Pointsåˆ†é¡
            print("ğŸ§© Entry Pointsåˆ†é¡ä¸­...")
            entry_points = await self._classify_entry_points(social_posts, search_queries)
            
            # Step 4: ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            print("ğŸ“ˆ ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”Ÿæˆä¸­...")
            heatmap_data = self._generate_heatmap_data(entry_points)
            
            # Step 5: æ©Ÿä¼šç™ºè¦‹
            opportunities = self._find_opportunities(entry_points)
            
            execution_time = time.time() - start_time
            
            result = {
                "framework_info": {
                    "id": self.framework_id,
                    "name": self.framework_name,
                    "version": self.version,
                    "execution_time": execution_time,
                    "timestamp": datetime.now().isoformat()
                },
                "input_parameters": {
                    "category": category,
                    "time_range": time_range
                },
                "entry_points": entry_points,
                "heatmap_data": heatmap_data,
                "opportunities": opportunities,
                "data_summary": {
                    "total_posts": len(social_posts),
                    "total_queries": len(search_queries),
                    "unique_entry_points": len(entry_points)
                },
                "insights": self._generate_insights(entry_points, opportunities),
                "success": True
            }
            
            print(f"âœ… åˆ†æå®Œäº† ({execution_time:.2f}ç§’)")
            return result
            
        except Exception as e:
            error_result = {
                "framework_info": {
                    "id": self.framework_id,
                    "name": self.framework_name,
                    "error": str(e)
                },
                "success": False,
                "execution_time": time.time() - start_time
            }
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return error_result
    
    async def _collect_social_posts(self, category: str, time_range: str) -> List[str]:
        """SNSæŠ•ç¨¿åé›†ï¼ˆã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ï¼‰"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯Twitter APIç­‰ã‚’ä½¿ç”¨
        sample_posts = [
            f"æœã®å¿™ã—ã„æ™‚ã«{category}ãŒæ¬²ã—ããªã‚‹",
            f"é‹å‹•å¾Œã«{category}ã‚’æ‘‚å–",
            f"ã‚¹ãƒˆãƒ¬ã‚¹æ„Ÿã˜ãŸæ™‚ã«{category}ã§ç™’ã•ã‚Œã‚‹",
            f"å¤œå¯ã‚‹å‰ã®{category}ã‚¿ã‚¤ãƒ ",
            f"ä»•äº‹ã®ä¼‘æ†©æ™‚é–“ã«{category}",
            f"å‹é”ã¨{category}ã‚’æ¥½ã—ã‚€",
            f"ä¸€äººã®æ™‚é–“ã«{category}",
            f"ç–²ã‚ŒãŸæ™‚ã“ã{category}",
            f"é ‘å¼µã£ãŸè‡ªåˆ†ã¸ã®{category}ã”è¤’ç¾",
            f"ç½ªæ‚ªæ„Ÿã‚’æ„Ÿã˜ãªãŒã‚‰ã‚‚{category}",
        ]
        
        await asyncio.sleep(0.5)  # APIå‘¼ã³å‡ºã—æ¨¡æ“¬
        return sample_posts
    
    async def _collect_search_queries(self, category: str) -> List[str]:
        """æ¤œç´¢ã‚¯ã‚¨ãƒªåé›†ï¼ˆã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼‰"""
        sample_queries = [
            f"{category} æœ",
            f"{category} é‹å‹•å¾Œ",
            f"{category} ã‚¹ãƒˆãƒ¬ã‚¹",
            f"{category} å¤œ",
            f"{category} ä¼‘æ†©",
            f"{category} ãŠã™ã™ã‚",
            f"{category} åŠ¹æœ",
            f"{category} ç¶šã‘ã‚‹æ–¹æ³•",
        ]
        
        await asyncio.sleep(0.3)
        return sample_queries
    
    async def _classify_entry_points(self, posts: List[str], queries: List[str]) -> Dict[str, Dict]:
        """Entry Pointsåˆ†é¡"""
        
        # æ–‡è„ˆã‚«ãƒ†ã‚´ãƒªå®šç¾©
        context_patterns = {
            "æœã®å¿™ã—ã„æ™‚": ["æœ", "å¿™ã—ã„", "æ€¥ã„ã§", "æ™‚é–“ãªã„"],
            "é‹å‹•å¾Œ": ["é‹å‹•", "ã‚¸ãƒ ", "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°", "æ±—"],
            "ã‚¹ãƒˆãƒ¬ã‚¹æ™‚": ["ã‚¹ãƒˆãƒ¬ã‚¹", "ç–²ã‚Œ", "ã‚¤ãƒ©ã‚¤ãƒ©", "ç™’ã—"],
            "å¤œã®ãƒªãƒ©ãƒƒã‚¯ã‚¹": ["å¤œ", "å¯ã‚‹å‰", "ãƒªãƒ©ãƒƒã‚¯ã‚¹", "ãã¤ã‚ã"],
            "ä»•äº‹ã®ä¼‘æ†©": ["ä¼‘æ†©", "ä»•äº‹", "ã‚ªãƒ•ã‚£ã‚¹", "ä¼šç¤¾"],
            "ã”è¤’ç¾ã‚¿ã‚¤ãƒ ": ["ã”è¤’ç¾", "é ‘å¼µã£ãŸ", "è‡ªåˆ†ã¸", "è´…æ²¢"],
            "ç½ªæ‚ªæ„Ÿ": ["ç½ªæ‚ªæ„Ÿ", "ã ã‚ã ", "ã¾ãŸ", "ã„ã‘ãªã„"],
            "ç¤¾äº¤": ["å‹é”", "ã¿ã‚“ãªã§", "ä¸€ç·’ã«", "ã‚·ã‚§ã‚¢"]
        }
        
        entry_points = {}
        all_text = posts + queries
        
        for context, keywords in context_patterns.items():
            frequency = 0
            sentiment_scores = []
            
            for text in all_text:
                if any(keyword in text for keyword in keywords):
                    frequency += 1
                    # ç°¡æ˜“æ„Ÿæƒ…åˆ†æï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯AIä½¿ç”¨ï¼‰
                    if any(negative in text for negative in ["ç–²ã‚Œ", "ã‚¹ãƒˆãƒ¬ã‚¹", "ç½ªæ‚ªæ„Ÿ"]):
                        sentiment_scores.append(0.3)
                    elif any(positive in text for positive in ["ã”è¤’ç¾", "ç™’ã—", "æ¥½ã—ã‚€"]):
                        sentiment_scores.append(0.8)
                    else:
                        sentiment_scores.append(0.5)
            
            if frequency > 0:
                entry_points[context] = {
                    "frequency": frequency,
                    "percentage": frequency / len(all_text) * 100,
                    "avg_sentiment": sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0.5,
                    "keywords": keywords
                }
        
        return entry_points
    
    def _generate_heatmap_data(self, entry_points: Dict[str, Dict]) -> Dict[str, Any]:
        """ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        
        # æ™‚é–“è»¸ x æ–‡è„ˆè»¸ã®ãƒãƒˆãƒªã‚¯ã‚¹
        time_slots = ["æœ", "æ˜¼", "å¤•æ–¹", "å¤œ"]
        contexts = list(entry_points.keys())
        
        # ç°¡æ˜“ãƒãƒƒãƒ”ãƒ³ã‚°
        time_context_mapping = {
            "æœ": ["æœã®å¿™ã—ã„æ™‚"],
            "æ˜¼": ["ä»•äº‹ã®ä¼‘æ†©", "ç¤¾äº¤"],
            "å¤•æ–¹": ["é‹å‹•å¾Œ", "ã‚¹ãƒˆãƒ¬ã‚¹æ™‚"],
            "å¤œ": ["å¤œã®ãƒªãƒ©ãƒƒã‚¯ã‚¹", "ã”è¤’ç¾ã‚¿ã‚¤ãƒ ", "ç½ªæ‚ªæ„Ÿ"]
        }
        
        heatmap_matrix = []
        for time_slot in time_slots:
            row = []
            for context in contexts:
                if context in time_context_mapping.get(time_slot, []):
                    intensity = entry_points[context]["frequency"] * entry_points[context]["avg_sentiment"]
                else:
                    intensity = 0
                row.append(intensity)
            heatmap_matrix.append(row)
        
        return {
            "matrix": heatmap_matrix,
            "time_slots": time_slots,
            "contexts": contexts
        }
    
    def _find_opportunities(self, entry_points: Dict[str, Dict]) -> List[Dict[str, Any]]:
        """æ©Ÿä¼šç™ºè¦‹"""
        opportunities = []
        
        # ä½é »åº¦ã ãŒé«˜æ„Ÿæƒ…ä¾¡å€¤ã®æ–‡è„ˆ
        for context, data in entry_points.items():
            if data["frequency"] < 3 and data["avg_sentiment"] > 0.6:
                opportunities.append({
                    "type": "underexplored_high_value",
                    "context": context,
                    "frequency": data["frequency"],
                    "sentiment": data["avg_sentiment"],
                    "opportunity": f"é«˜æ„Ÿæƒ…ä¾¡å€¤ã ãŒæœªé–‹æ‹“ã®'{context}'æ–‡è„ˆ"
                })
        
        # é«˜é »åº¦ã ãŒä½æ„Ÿæƒ…ä¾¡å€¤ï¼ˆæ”¹å–„æ©Ÿä¼šï¼‰
        for context, data in entry_points.items():
            if data["frequency"] >= 3 and data["avg_sentiment"] < 0.4:
                opportunities.append({
                    "type": "high_frequency_low_satisfaction",
                    "context": context,
                    "frequency": data["frequency"],
                    "sentiment": data["avg_sentiment"],
                    "opportunity": f"é »å‡ºã ãŒæº€è¶³åº¦ä½ã„'{context}'ã®æ”¹å–„æ©Ÿä¼š"
                })
        
        return opportunities
    
    def _generate_insights(self, entry_points: Dict[str, Dict], 
                          opportunities: List[Dict[str, Any]]) -> List[str]:
        """æ´å¯Ÿç”Ÿæˆ"""
        insights = []
        
        # æœ€é »å‡ºEntry Point
        if entry_points:
            top_context = max(entry_points.items(), key=lambda x: x[1]["frequency"])
            insights.append(f"æœ€é »å‡ºEntry Point: '{top_context[0]}' ({top_context[1]['frequency']}å›)")
        
        # æœ€é«˜æ„Ÿæƒ…ä¾¡å€¤Entry Point
        if entry_points:
            best_sentiment = max(entry_points.items(), key=lambda x: x[1]["avg_sentiment"])
            insights.append(f"æœ€é«˜æ„Ÿæƒ…ä¾¡å€¤: '{best_sentiment[0]}' (æ„Ÿæƒ…ã‚¹ã‚³ã‚¢{best_sentiment[1]['avg_sentiment']:.2f})")
        
        # æ©Ÿä¼šæ•°
        insights.append(f"ç™ºè¦‹ã•ã‚ŒãŸæ©Ÿä¼š: {len(opportunities)}ä»¶")
        
        # æœªé–‹æ‹“æ©Ÿä¼š
        underexplored = [opp for opp in opportunities if opp["type"] == "underexplored_high_value"]
        if underexplored:
            insights.append(f"æœªé–‹æ‹“é«˜ä¾¡å€¤æ–‡è„ˆ: {len(underexplored)}ä»¶")
        
        return insights

# [ç¶šã - save_results, print_summary, mainé–¢æ•°ã¯åŒæ§˜ã®ãƒ‘ã‚¿ãƒ¼ãƒ³]

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
SAMPLE_DATA = {
    "category": "å¥åº·é£Ÿå“",
    "time_range": "30d"
}

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="Category Entry Points åˆ†æãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯")
    parser.add_argument("--category", default=SAMPLE_DATA["category"], help="åˆ†æã‚«ãƒ†ã‚´ãƒª")
    parser.add_argument("--timerange", default=SAMPLE_DATA["time_range"], help="åˆ†ææœŸé–“")
    parser.add_argument("--config", help="è¨­å®šJSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")
    
    args = parser.parse_args()
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    if args.config and os.path.exists(args.config):
        with open(args.config, 'r', encoding='utf-8') as f:
            config = json.load(f)
        category = config.get("category", args.category)
        time_range = config.get("time_range", args.timerange)
    else:
        category = args.category
        time_range = args.timerange
    
    # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å®Ÿè¡Œ
    framework = CategoryEntryPointsFramework()
    result = await framework.collect_data(category, time_range)
    
    # çµæœä¿å­˜ãƒ»è¡¨ç¤º
    framework.save_results(result)
    framework.print_summary(result)

if __name__ == "__main__":
    asyncio.run(main())
```

## 3. å…±é€šè¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³

### 3.1 ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ çµ±ä¸€

```python
# å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å…±é€šã®ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ 
"""
#!/usr/bin/env python3
ãƒ•ãƒ¬ãƒ¼ãƒ X: [ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å]
å®Ÿè¡Œæ–¹æ³•: python framework_XX_name.py
"""

import asyncio
import json
import time
import argparse
from datetime import datetime
from typing import List, Dict, Any
import os

class FrameworkXXXX:
    def __init__(self):
        self.framework_id = XX
        self.framework_name = "Framework Name"
        self.version = "1.0.0"
        self.output_dir = f"output_framework_{self.framework_id:02d}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        os.makedirs(self.output_dir, exist_ok=True)
    
    async def collect_data(self, **kwargs) -> Dict[str, Any]:
        """ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»åˆ†æãƒ­ã‚¸ãƒƒã‚¯"""
        pass
    
    def save_results(self, result: Dict[str, Any]) -> None:
        """çµæœä¿å­˜ï¼ˆJSON, CSV, ç”»åƒç­‰ï¼‰"""
        pass
    
    def print_summary(self, result: Dict[str, Any]) -> None:
        """çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        pass

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿å®šç¾©
SAMPLE_DATA = {...}

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯èª¬æ˜")
    # å¼•æ•°å®šç¾©
    args = parser.parse_args()
    
    # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å®Ÿè¡Œ
    framework = FrameworkXXXX()
    result = await framework.collect_data(**params)
    
    # çµæœå‡¦ç†
    framework.save_results(result)
    framework.print_summary(result)

if __name__ == "__main__":
    asyncio.run(main())
```

### 3.2 çµ±ä¸€å‡ºåŠ›å½¢å¼

```bash
# å®Ÿè¡Œä¾‹
$ python framework_05_share_of_search.py --brand "ã‚³ã‚«ã‚³ãƒ¼ãƒ©" --competitors "ãƒšãƒ—ã‚·" "ä¼Šå³è¡›é–€"

ğŸš€ Share of Search v1.0.0 é–‹å§‹
ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: output_framework_05_20241201_143022
ğŸ“Š åˆ†æé–‹å§‹: ã‚³ã‚«ã‚³ãƒ¼ãƒ© vs ['ãƒšãƒ—ã‚·', 'ä¼Šå³è¡›é–€']
ğŸ”— Google Trends APIæ¥ç¶šä¸­...
ğŸ” æ¤œç´¢ãƒ‡ãƒ¼ã‚¿å–å¾—: ['ã‚³ã‚«ã‚³ãƒ¼ãƒ©', 'ãƒšãƒ—ã‚·', 'ä¼Šå³è¡›é–€']
ğŸ§® Share of Search è¨ˆç®—ä¸­...
âœ… åˆ†æå®Œäº† (12.34ç§’)
ğŸ’¾ çµæœä¿å­˜: output_framework_05_20241201_143022/sos_analysis_result.json
ğŸ“Š CSVä¿å­˜: output_framework_05_20241201_143022/share_of_search.csv
ğŸ“ˆ ã‚°ãƒ©ãƒ•ä¿å­˜: output_framework_05_20241201_143022/sos_analysis_chart.png

ğŸ“‹ Share of Search å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼
==================================================
ğŸ¯ å¯¾è±¡ãƒ–ãƒ©ãƒ³ãƒ‰: ã‚³ã‚«ã‚³ãƒ¼ãƒ©
ğŸ†š ç«¶åˆãƒ–ãƒ©ãƒ³ãƒ‰: ãƒšãƒ—ã‚·, ä¼Šå³è¡›é–€
â±ï¸  å®Ÿè¡Œæ™‚é–“: 12.34ç§’

ğŸ“Š Share of Searchçµæœ:
  ã‚³ã‚«ã‚³ãƒ¼ãƒ©: 45.2%
  ãƒšãƒ—ã‚·: 28.7%
  ä¼Šå³è¡›é–€: 26.1%

ğŸ† ç«¶åˆãƒã‚¸ã‚·ãƒ§ãƒ³:
  é †ä½: 1/3ä½
  å¸‚å ´ãƒªãƒ¼ãƒ€ãƒ¼: ã‚³ã‚«ã‚³ãƒ¼ãƒ©
  ãƒªãƒ¼ãƒ€ãƒ¼ã¨ã®å·®: 0.0%

ğŸ’¡ ä¸»è¦æ´å¯Ÿ:
  â€¢ æ¤œç´¢ã‚·ã‚§ã‚¢1ä½ã¯ã‚³ã‚«ã‚³ãƒ¼ãƒ©ã§45.2%
  â€¢ ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰: ã‚³ã‚«ã‚³ãƒ¼ãƒ©
  â€¢ ç«¶åˆæ¿€æˆ¦çŠ¶æ³

ğŸ“ è©³ç´°çµæœã¯ output_framework_05_20241201_143022 ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ
```

## 4. å®Ÿè£…å„ªå…ˆé †ãƒªã‚¹ãƒˆ

### 4.1 å³åº§å®Ÿè£…å¯èƒ½ï¼ˆP0ï¼‰

1. **framework_05_share_of_search.py** 
   - Google Trends APIä½¿ç”¨
   - æŠ€è¡“ãƒªã‚¹ã‚¯ä½ãƒ»åŠ¹æœæ˜ç¢º

2. **framework_12_copy_dna.py**
   - ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã®ã¿
   - å¤–éƒ¨APIä¾å­˜ãªã—

3. **framework_03_dba_score.py**
   - èª¿æŸ»ãƒ‡ãƒ¼ã‚¿æ¨¡æ“¬å¯èƒ½

### 4.2 ä¸­æœŸå®Ÿè£…ï¼ˆP1ï¼‰

4. **framework_02_cep_distribution.py**
5. **framework_13_meme_emoji.py**
6. **framework_01_brand_key.py**

### 4.3 é«˜åº¦å®Ÿè£…ï¼ˆP2ï¼‰

7. **framework_09_emotion_eye.py**
8. **framework_11_shopper_journey.py**
9. **framework_20_media_mix.py**

## 5. å®Ÿè¡Œãƒ»ãƒ†ã‚¹ãƒˆæ–¹æ³•

### 5.1 åŸºæœ¬å®Ÿè¡Œ

```bash
# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå®Ÿè¡Œ
python framework_05_share_of_search.py

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŒ‡å®šå®Ÿè¡Œ
python framework_05_share_of_search.py --brand "Apple" --competitors "Samsung" "Google"

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½¿ç”¨
python framework_05_share_of_search.py --config myconfig.json
```

### 5.2 è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä¾‹

```json
{
  "brand_name": "iPhone",
  "competitors": ["Galaxy", "Pixel", "Xperia"],
  "category": "ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³",
  "time_range": "today 6-m"
}
```

### 5.3 ãƒãƒƒãƒå®Ÿè¡Œ

```bash
# è¤‡æ•°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯é€£ç¶šå®Ÿè¡Œ
python framework_05_share_of_search.py
python framework_02_cep_distribution.py
python framework_13_meme_emoji.py
```

---

ã“ã®è¨­è¨ˆã«ã‚ˆã‚Šã€**å„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãŒå®Œå…¨ç‹¬ç«‹**ã§å‹•ä½œã—ã€`python framework_XX.py`ã§å³åº§ã«å®Ÿè¡Œå¯èƒ½ã«ãªã‚Šã¾ã™ï¼ 