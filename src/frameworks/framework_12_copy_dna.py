#!/usr/bin/env python3
"""
ãƒ•ãƒ¬ãƒ¼ãƒ 12: Copy DNA Audit (ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°èªå½™ãƒ»ãƒªã‚ºãƒ ãƒ»æ§‹æ–‡åˆ†æ)
å®Ÿè¡Œæ–¹æ³•: python framework_12_copy_dna.py
"""

import asyncio
import json
import time
import argparse
from datetime import datetime
from typing import List, Dict, Any, Tuple
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import sys
import re
from collections import Counter, defaultdict
import numpy as np

# ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒã‚§ãƒƒã‚¯
try:
    import jieba
    import wordcloud
except ImportError:
    print("âŒ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
    print("pip install jieba wordcloud matplotlib seaborn pandas numpy")
    sys.exit(1)

class CopyDNAFramework:
    """Copy DNA Audit ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ - ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ç‰ˆ"""
    
    def __init__(self):
        self.framework_id = 12
        self.framework_name = "Copy DNA Audit"
        self.version = "1.0.0"
        self.output_dir = f"data/output/output_framework_12_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(self.output_dir, exist_ok=True)
        
        print(f"ğŸš€ {self.framework_name} v{self.version} é–‹å§‹")
        print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
        
        # æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆåˆ†æç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.hiragana_pattern = re.compile(r'[ã²ã‚‰ãŒãª]+')
        self.katakana_pattern = re.compile(r'[ã‚«ã‚¿ã‚«ãƒŠ]+')
        self.kanji_pattern = re.compile(r'[ä¸€-é¾¯]+')
        self.punctuation_pattern = re.compile(r'[ã€‚ã€ï¼ï¼Ÿâ€¦ãƒ»â€»]')
        
    async def collect_data(self, copy_samples: List[str], brand_name: str = "Unknown") -> Dict[str, Any]:
        """Copy DNA ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»åˆ†æ"""
        
        start_time = time.time()
        print(f"\nğŸ“Š åˆ†æé–‹å§‹: {len(copy_samples)}ä»¶ã®ã‚³ãƒ”ãƒ¼åˆ†æ")
        
        try:
            # Step 1: åŸºæœ¬çµ±è¨ˆåˆ†æ
            print("ğŸ“ åŸºæœ¬çµ±è¨ˆåˆ†æä¸­...")
            basic_stats = self._analyze_basic_statistics(copy_samples)
            
            # Step 2: èªå½™åˆ†æ
            print("ğŸ”¤ èªå½™åˆ†æä¸­...")
            vocabulary_analysis = await self._analyze_vocabulary(copy_samples)
            
            # Step 3: ãƒªã‚ºãƒ ãƒ»éŸ³éŸ»åˆ†æ
            print("ğŸµ ãƒªã‚ºãƒ åˆ†æä¸­...")
            rhythm_analysis = self._analyze_rhythm_patterns(copy_samples)
            
            # Step 4: æ§‹æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            print("ğŸ—ï¸ æ§‹æ–‡åˆ†æä¸­...")
            syntax_analysis = self._analyze_syntax_patterns(copy_samples)
            
            # Step 5: æ„Ÿæƒ…ãƒ»ãƒˆãƒ¼ãƒ³åˆ†æ
            print("ğŸ˜Š æ„Ÿæƒ…ãƒˆãƒ¼ãƒ³åˆ†æä¸­...")
            emotion_analysis = self._analyze_emotional_tone(copy_samples)
            
            # Step 6: DNAãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º
            print("ğŸ§¬ DNAãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡ºä¸­...")
            dna_patterns = self._extract_dna_patterns(
                vocabulary_analysis, rhythm_analysis, syntax_analysis, emotion_analysis
            )
            
            execution_time = time.time() - start_time
            
            result = {
                "framework_info": {
                    "id": self.framework_id,
                    "name": self.framework_name,
                    "version": self.version,
                    "execution_time": execution_time,
                    "timestamp": datetime.now().isoformat()
                },
                "input_parameters": {
                    "brand_name": brand_name,
                    "copy_count": len(copy_samples),
                    "sample_copies": copy_samples[:3]  # æœ€åˆã®3ä»¶ã‚’ã‚µãƒ³ãƒ—ãƒ«ã¨ã—ã¦ä¿å­˜
                },
                "basic_statistics": basic_stats,
                "vocabulary_analysis": vocabulary_analysis,
                "rhythm_analysis": rhythm_analysis,
                "syntax_analysis": syntax_analysis,
                "emotion_analysis": emotion_analysis,
                "dna_patterns": dna_patterns,
                "insights": self._generate_insights(basic_stats, vocabulary_analysis, dna_patterns),
                "success": True
            }
            
            print(f"âœ… åˆ†æå®Œäº† ({execution_time:.2f}ç§’)")
            return result
            
        except Exception as e:
            error_result = {
                "framework_info": {
                    "id": self.framework_id,
                    "name": self.framework_name,
                    "error": str(e)
                },
                "success": False,
                "execution_time": time.time() - start_time
            }
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return error_result
    
    def _analyze_basic_statistics(self, copies: List[str]) -> Dict[str, Any]:
        """åŸºæœ¬çµ±è¨ˆåˆ†æ"""
        char_counts = [len(copy) for copy in copies]
        word_counts = [len(copy.split()) for copy in copies]
        sentence_counts = [len(re.split(r'[ã€‚ï¼ï¼Ÿ]', copy)) for copy in copies]
        
        return {
            "total_copies": len(copies),
            "character_stats": {
                "avg": np.mean(char_counts),
                "median": np.median(char_counts),
                "min": min(char_counts),
                "max": max(char_counts),
                "std": np.std(char_counts)
            },
            "word_stats": {
                "avg": np.mean(word_counts),
                "median": np.median(word_counts),
                "std": np.std(word_counts)
            },
            "sentence_stats": {
                "avg": np.mean(sentence_counts),
                "median": np.median(sentence_counts),
                "std": np.std(sentence_counts)
            }
        }
    
    async def _analyze_vocabulary(self, copies: List[str]) -> Dict[str, Any]:
        """èªå½™åˆ†æ"""
        all_text = ' '.join(copies)
        
        # å½¢æ…‹ç´ è§£æï¼ˆç°¡æ˜“ç‰ˆï¼‰
        words = []
        for copy in copies:
            # ã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ã®åˆ†é›¢
            hiragana_words = re.findall(r'[ã²ã‚‰ãŒãª]+', copy)
            katakana_words = re.findall(r'[ã‚¡-ãƒ¶]+', copy)
            kanji_words = re.findall(r'[ä¸€-é¾¯]+', copy)
            
            words.extend(hiragana_words + katakana_words + kanji_words)
        
        # èªå½™çµ±è¨ˆ
        word_freq = Counter(words)
        unique_words = len(word_freq)
        total_words = sum(word_freq.values())
        
        # N-gramåˆ†æ
        bigrams = self._extract_ngrams(all_text, 2)
        trigrams = self._extract_ngrams(all_text, 3)
        
        # æ–‡å­—ç¨®åˆ¥åˆ†æ
        char_type_ratio = self._analyze_character_types(all_text)
        
        return {
            "vocabulary_size": unique_words,
            "total_words": total_words,
            "vocabulary_richness": unique_words / total_words if total_words > 0 else 0,
            "top_words": dict(word_freq.most_common(20)),
            "top_bigrams": dict(Counter(bigrams).most_common(10)),
            "top_trigrams": dict(Counter(trigrams).most_common(10)),
            "character_type_ratio": char_type_ratio,
            "rare_words": [word for word, freq in word_freq.items() if freq == 1][:10]
        }
    
    def _extract_ngrams(self, text: str, n: int) -> List[str]:
        """N-gramæŠ½å‡º"""
        # ç°¡æ˜“ç‰ˆï¼šæ–‡å­—å˜ä½ã®N-gram
        cleaned_text = re.sub(r'[^\w]', '', text)
        return [cleaned_text[i:i+n] for i in range(len(cleaned_text)-n+1)]
    
    def _analyze_character_types(self, text: str) -> Dict[str, float]:
        """æ–‡å­—ç¨®åˆ¥æ¯”ç‡åˆ†æ"""
        total_chars = len(re.sub(r'\s', '', text))
        
        hiragana_count = len(re.findall(r'[ã²ã‚‰ãŒãª]', text))
        katakana_count = len(re.findall(r'[ã‚¡-ãƒ¶]', text))
        kanji_count = len(re.findall(r'[ä¸€-é¾¯]', text))
        number_count = len(re.findall(r'[0-9]', text))
        alphabet_count = len(re.findall(r'[a-zA-Z]', text))
        
        return {
            "hiragana_ratio": hiragana_count / total_chars if total_chars > 0 else 0,
            "katakana_ratio": katakana_count / total_chars if total_chars > 0 else 0,
            "kanji_ratio": kanji_count / total_chars if total_chars > 0 else 0,
            "number_ratio": number_count / total_chars if total_chars > 0 else 0,
            "alphabet_ratio": alphabet_count / total_chars if total_chars > 0 else 0
        }
    
    def _analyze_rhythm_patterns(self, copies: List[str]) -> Dict[str, Any]:
        """ãƒªã‚ºãƒ ãƒ»éŸ³éŸ»ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        
        # å¥èª­ç‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
        punctuation_patterns = []
        for copy in copies:
            pattern = re.findall(r'[ã€‚ã€ï¼ï¼Ÿâ€¦ãƒ»â€»]', copy)
            punctuation_patterns.append(''.join(pattern))
        
        # æ–‡æœ«ãƒ‘ã‚¿ãƒ¼ãƒ³
        ending_patterns = []
        for copy in copies:
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', copy)
            for sentence in sentences:
                if sentence.strip():
                    ending = sentence.strip()[-2:] if len(sentence.strip()) >= 2 else sentence.strip()
                    ending_patterns.append(ending)
        
        # é•·æ–‡ãƒ»çŸ­æ–‡æ¯”ç‡
        sentence_lengths = []
        for copy in copies:
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', copy)
            for sentence in sentences:
                if sentence.strip():
                    sentence_lengths.append(len(sentence.strip()))
        
        # ãƒªã‚ºãƒ ã‚¿ã‚¤ãƒ—åˆ†é¡
        rhythm_types = self._classify_rhythm_types(copies)
        
        return {
            "punctuation_frequency": Counter(punctuation_patterns),
            "ending_patterns": dict(Counter(ending_patterns).most_common(10)),
            "sentence_length_distribution": {
                "short": len([s for s in sentence_lengths if s <= 15]),
                "medium": len([s for s in sentence_lengths if 16 <= s <= 30]),
                "long": len([s for s in sentence_lengths if s > 30])
            },
            "avg_sentence_length": np.mean(sentence_lengths) if sentence_lengths else 0,
            "rhythm_types": rhythm_types,
            "exclamation_ratio": sum(copy.count('ï¼') for copy in copies) / len(copies),
            "question_ratio": sum(copy.count('ï¼Ÿ') for copy in copies) / len(copies)
        }
    
    def _classify_rhythm_types(self, copies: List[str]) -> Dict[str, int]:
        """ãƒªã‚ºãƒ ã‚¿ã‚¤ãƒ—åˆ†é¡"""
        rhythm_counts = defaultdict(int)
        
        for copy in copies:
            # ä½“è¨€æ­¢ã‚
            if re.search(r'[ã€‚ï¼ï¼Ÿ]$', copy) and not re.search(r'[ã§ã™|ã¾ã™|ã§ã‚ã‚‹]', copy[-10:]):
                rhythm_counts["ä½“è¨€æ­¢ã‚"] += 1
            
            # ç–‘å•å½¢
            if 'ï¼Ÿ' in copy or re.search(r'[ã§ã™ã‹|ã¾ã™ã‹|ã§ã—ã‚‡ã†ã‹]', copy):
                rhythm_counts["ç–‘å•å½¢"] += 1
            
            # æ„Ÿå˜†å½¢
            if 'ï¼' in copy or re.search(r'[ã ï¼|ã§ã™ï¼|ã¾ã™ï¼]', copy):
                rhythm_counts["æ„Ÿå˜†å½¢"] += 1
            
            # ä¸å¯§èª
            if re.search(r'[ã§ã™|ã¾ã™]', copy):
                rhythm_counts["ä¸å¯§èª"] += 1
            
            # çŸ­æ–‡é€£ç¶š
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', copy)
            short_sentences = [s for s in sentences if len(s.strip()) <= 10]
            if len(short_sentences) >= 2:
                rhythm_counts["çŸ­æ–‡é€£ç¶š"] += 1
        
        return dict(rhythm_counts)
    
    def _analyze_syntax_patterns(self, copies: List[str]) -> Dict[str, Any]:
        """æ§‹æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        
        # æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³
        sentence_patterns = []
        modifier_patterns = []
        
        for copy in copies:
            # ä¿®é£¾èªãƒ‘ã‚¿ãƒ¼ãƒ³
            modifiers = re.findall(r'[æ–°ã—ã„|ç¾ã—ã„|ç´ æ™´ã‚‰ã—ã„|æœ€é«˜ã®|ç©¶æ¥µã®|é©æ–°çš„ãª]', copy)
            modifier_patterns.extend(modifiers)
            
            # æ–‡å‹åˆ¤å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰
            if re.search(r'[ã§ã™|ã¾ã™]$', copy):
                sentence_patterns.append("ä¸å¯§èª")
            elif re.search(r'[ã |ã§ã‚ã‚‹]$', copy):
                sentence_patterns.append("æ–­å®šèª")
            elif copy.endswith('ã€‚'):
                sentence_patterns.append("æ™®é€šèª")
        
        # æ¥ç¶šè©åˆ†æ
        conjunctions = re.findall(r'[ãã—ã¦|ã¾ãŸ|ã—ã‹ã—|ã ã‹ã‚‰|ãªã®ã§|ã¤ã¾ã‚Š|ä¾‹ãˆã°]', ' '.join(copies))
        
        # å¼·èª¿è¡¨ç¾
        emphasis_patterns = []
        for copy in copies:
            if re.search(r'[ã¨ã¦ã‚‚|éå¸¸ã«|ã‹ãªã‚Š|çµ¶å¯¾ã«|å¿…ãš|ãã£ã¨]', copy):
                emphasis_patterns.append("ç¨‹åº¦å‰¯è©")
            if 'ï¼' in copy:
                emphasis_patterns.append("æ„Ÿå˜†ç¬¦")
            if re.search(r'[ã€œã€œ|â€¦|ï¼ï¼]', copy):
                emphasis_patterns.append("è¨˜å·å¼·èª¿")
        
        return {
            "sentence_patterns": dict(Counter(sentence_patterns)),
            "modifier_frequency": dict(Counter(modifier_patterns)),
            "conjunction_usage": dict(Counter(conjunctions)),
            "emphasis_patterns": dict(Counter(emphasis_patterns)),
            "complex_sentence_ratio": self._calculate_complex_sentence_ratio(copies)
        }
    
    def _calculate_complex_sentence_ratio(self, copies: List[str]) -> float:
        """è¤‡æ–‡æ¯”ç‡è¨ˆç®—"""
        complex_count = 0
        total_sentences = 0
        
        for copy in copies:
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', copy)
            for sentence in sentences:
                if sentence.strip():
                    total_sentences += 1
                    # è¤‡æ–‡åˆ¤å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰
                    if re.search(r'[ãŒã€|ã‘ã‚Œã©|ã®ã§|ã‹ã‚‰|ãŸã‚]', sentence):
                        complex_count += 1
        
        return complex_count / total_sentences if total_sentences > 0 else 0
    
    def _analyze_emotional_tone(self, copies: List[str]) -> Dict[str, Any]:
        """æ„Ÿæƒ…ãƒ»ãƒˆãƒ¼ãƒ³åˆ†æ"""
        
        # æ„Ÿæƒ…èªè¾æ›¸ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        positive_words = ['å¬‰ã—ã„', 'æ¥½ã—ã„', 'ç´ æ™´ã‚‰ã—ã„', 'æœ€é«˜', 'è‰¯ã„', 'ç¾å‘³ã—ã„', 'å¿«é©', 'å®‰å¿ƒ']
        negative_words = ['æ‚²ã—ã„', 'å›°ã‚‹', 'å¿ƒé…', 'ä¸å®‰', 'è¾›ã„', 'è‹¦ã—ã„', 'æ‚ªã„', 'ãƒ€ãƒ¡']
        urgency_words = ['ä»Šã™ã', 'æ€¥ã„ã§', 'é™å®š', 'ç·Šæ€¥', 'ãŠæ€¥ã', 'ã™ãã«']
        luxury_words = ['é«˜ç´š', 'è´…æ²¢', 'ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ', 'ç‰¹åˆ¥', 'é™å®š', 'VIP']
        
        emotion_scores = {
            'positive': 0,
            'negative': 0,
            'urgency': 0,
            'luxury': 0
        }
        
        for copy in copies:
            for word in positive_words:
                emotion_scores['positive'] += copy.count(word)
            for word in negative_words:
                emotion_scores['negative'] += copy.count(word)
            for word in urgency_words:
                emotion_scores['urgency'] += copy.count(word)
            for word in luxury_words:
                emotion_scores['luxury'] += copy.count(word)
        
        # ãƒˆãƒ¼ãƒ³åˆ†é¡
        tone_classification = self._classify_tones(copies)
        
        return {
            "emotion_scores": emotion_scores,
            "dominant_emotion": max(emotion_scores.items(), key=lambda x: x[1])[0],
            "tone_classification": tone_classification,
            "emotional_intensity": sum(emotion_scores.values()) / len(copies)
        }
    
    def _classify_tones(self, copies: List[str]) -> Dict[str, int]:
        """ãƒˆãƒ¼ãƒ³åˆ†é¡"""
        tone_counts = defaultdict(int)
        
        for copy in copies:
            copy_lower = copy.lower()
            
            # ãƒ•ã‚©ãƒ¼ãƒãƒ«åº¦
            if re.search(r'[ã„ãŸã—ã¾ã™|ã•ã›ã¦ã„ãŸã ã|ã”ã–ã„ã¾ã™]', copy):
                tone_counts["ãƒ•ã‚©ãƒ¼ãƒãƒ«"] += 1
            elif re.search(r'[ã ã‚ˆ|ã ã­|ã˜ã‚ƒã‚“]', copy):
                tone_counts["ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«"] += 1
            
            # èª¬å¾—ã‚¿ã‚¤ãƒ—
            if re.search(r'[è¨¼æ˜|å®Ÿè¨¼|ãƒ‡ãƒ¼ã‚¿|ç ”ç©¶]', copy):
                tone_counts["è«–ç†çš„"] += 1
            elif re.search(r'[æ„Ÿå‹•|ä½“é¨“|ç‰©èª]', copy):
                tone_counts["æ„Ÿæƒ…çš„"] += 1
            
            # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
            if re.search(r'[ã‚ãªãŸ|ãŠå®¢æ§˜|çš†æ§˜]', copy):
                tone_counts["å€‹äººçš„"] += 1
            elif re.search(r'[ç¤¾ä¼š|ä¸–ç•Œ|æœªæ¥]', copy):
                tone_counts["ç¤¾ä¼šçš„"] += 1
        
        return dict(tone_counts)
    
    def _extract_dna_patterns(self, vocab: Dict, rhythm: Dict, syntax: Dict, emotion: Dict) -> Dict[str, Any]:
        """DNAãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º"""
        
        # èªå½™DNAãƒ‘ã‚¿ãƒ¼ãƒ³
        vocab_dna = {
            "signature_words": list(vocab["top_words"].keys())[:5],
            "vocabulary_richness_level": "é«˜" if vocab["vocabulary_richness"] > 0.7 else "ä¸­" if vocab["vocabulary_richness"] > 0.4 else "ä½",
            "preferred_bigrams": list(vocab["top_bigrams"].keys())[:3]
        }
        
        # ãƒªã‚ºãƒ DNAãƒ‘ã‚¿ãƒ¼ãƒ³
        rhythm_dna = {
            "preferred_sentence_length": "çŸ­æ–‡" if rhythm["avg_sentence_length"] < 15 else "é•·æ–‡" if rhythm["avg_sentence_length"] > 30 else "ä¸­æ–‡",
            "dominant_rhythm": max(rhythm["rhythm_types"].items(), key=lambda x: x[1])[0] if rhythm["rhythm_types"] else "æ¨™æº–",
            "punctuation_style": "æ„Ÿå˜†å¤šç”¨" if rhythm["exclamation_ratio"] > 0.5 else "ç–‘å•å¤šç”¨" if rhythm["question_ratio"] > 0.3 else "æ¨™æº–"
        }
        
        # æ§‹æ–‡DNAãƒ‘ã‚¿ãƒ¼ãƒ³
        syntax_dna = {
            "sentence_style": max(syntax["sentence_patterns"].items(), key=lambda x: x[1])[0] if syntax["sentence_patterns"] else "æ¨™æº–",
            "complexity_level": "é«˜" if syntax["complex_sentence_ratio"] > 0.6 else "ä½" if syntax["complex_sentence_ratio"] < 0.3 else "ä¸­",
            "emphasis_preference": max(syntax["emphasis_patterns"].items(), key=lambda x: x[1])[0] if syntax["emphasis_patterns"] else "ãªã—"
        }
        
        # æ„Ÿæƒ…DNAãƒ‘ã‚¿ãƒ¼ãƒ³
        emotion_dna = {
            "emotional_tendency": emotion["dominant_emotion"],
            "intensity_level": "é«˜" if emotion["emotional_intensity"] > 2 else "ä½" if emotion["emotional_intensity"] < 0.5 else "ä¸­",
            "tone_preference": max(emotion["tone_classification"].items(), key=lambda x: x[1])[0] if emotion["tone_classification"] else "ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«"
        }
        
        # çµ±åˆDNAãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
        dna_profile = f"{vocab_dna['vocabulary_richness_level']}èªå½™Ã—{rhythm_dna['preferred_sentence_length']}Ã—{emotion_dna['emotional_tendency']}ãƒˆãƒ¼ãƒ³"
        
        return {
            "vocabulary_dna": vocab_dna,
            "rhythm_dna": rhythm_dna,
            "syntax_dna": syntax_dna,
            "emotion_dna": emotion_dna,
            "integrated_dna_profile": dna_profile
        }
    
    def _generate_insights(self, basic_stats: Dict, vocab: Dict, dna: Dict) -> List[str]:
        """æ´å¯Ÿç”Ÿæˆ"""
        insights = []
        
        # åŸºæœ¬çµ±è¨ˆæ´å¯Ÿ
        avg_chars = basic_stats["character_stats"]["avg"]
        if avg_chars < 20:
            insights.append("çŸ­æ–‡ä¸»ä½“ã®ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆãªã‚³ãƒ”ãƒ¼")
        elif avg_chars > 50:
            insights.append("é•·æ–‡ã§ã®è©³ç´°èª¬æ˜å‹ã‚³ãƒ”ãƒ¼")
        
        # èªå½™æ´å¯Ÿ
        if vocab["vocabulary_richness"] > 0.7:
            insights.append("èªå½™ã®å¤šæ§˜æ€§ãŒé«˜ãè¡¨ç¾è±Šã‹")
        elif vocab["vocabulary_richness"] < 0.3:
            insights.append("ã‚·ãƒ³ãƒ—ãƒ«ãªèªå½™ã§çµ±ä¸€æ„Ÿã‚’é‡è¦–")
        
        # DNAãƒ‘ã‚¿ãƒ¼ãƒ³æ´å¯Ÿ
        dna_profile = dna["integrated_dna_profile"]
        insights.append(f"DNAãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«: {dna_profile}")
        
        # å¼·ã¿ã¨ç‰¹å¾´
        if dna["emotion_dna"]["emotional_tendency"] == "positive":
            insights.append("ãƒã‚¸ãƒ†ã‚£ãƒ–æ„Ÿæƒ…ã§ã®ãƒ–ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°é‡è¦–")
        
        if dna["rhythm_dna"]["dominant_rhythm"] == "ä½“è¨€æ­¢ã‚":
            insights.append("ä½“è¨€æ­¢ã‚ã«ã‚ˆã‚‹ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆé‡è¦–")
        
        return insights
    
    def save_results(self, result: Dict[str, Any]) -> None:
        """çµæœä¿å­˜"""
        # JSONä¿å­˜
        json_path = os.path.join(self.output_dir, "copy_dna_analysis_result.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ çµæœä¿å­˜: {json_path}")
        
        # CSVä¿å­˜ï¼ˆèªå½™é »åº¦ï¼‰
        if result.get("success") and "vocabulary_analysis" in result:
            vocab_df = pd.DataFrame(list(result["vocabulary_analysis"]["top_words"].items()), 
                                  columns=["Word", "Frequency"])
            csv_path = os.path.join(self.output_dir, "vocabulary_frequency.csv")
            vocab_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"ğŸ“Š èªå½™é »åº¦CSVä¿å­˜: {csv_path}")
        
        # å¯è¦–åŒ–ã‚°ãƒ©ãƒ•ä¿å­˜
        if result.get("success"):
            self._save_visualization(result)
    
    def _save_visualization(self, result: Dict[str, Any]) -> None:
        """å¯è¦–åŒ–ã‚°ãƒ©ãƒ•ç”Ÿæˆãƒ»ä¿å­˜"""
        try:
            plt.style.use('default')
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
            
            # 1. èªå½™é »åº¦ã‚°ãƒ©ãƒ•
            vocab_data = result["vocabulary_analysis"]["top_words"]
            words = list(vocab_data.keys())[:10]
            freqs = list(vocab_data.values())[:10]
            
            ax1.bar(words, freqs)
            ax1.set_title("Top 10 èªå½™é »åº¦")
            ax1.set_ylabel("é »åº¦")
            ax1.tick_params(axis='x', rotation=45)
            
            # 2. æ–‡å­—ç¨®åˆ¥æ¯”ç‡
            char_ratio = result["vocabulary_analysis"]["character_type_ratio"]
            ax2.pie(char_ratio.values(), labels=char_ratio.keys(), autopct='%1.1f%%')
            ax2.set_title("æ–‡å­—ç¨®åˆ¥æ¯”ç‡")
            
            # 3. ãƒªã‚ºãƒ ãƒ‘ã‚¿ãƒ¼ãƒ³
            rhythm_types = result["rhythm_analysis"]["rhythm_types"]
            if rhythm_types:
                ax3.bar(rhythm_types.keys(), rhythm_types.values())
                ax3.set_title("ãƒªã‚ºãƒ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†å¸ƒ")
                ax3.set_ylabel("å‡ºç¾å›æ•°")
                ax3.tick_params(axis='x', rotation=45)
            
            # 4. æ„Ÿæƒ…ã‚¹ã‚³ã‚¢
            emotion_scores = result["emotion_analysis"]["emotion_scores"]
            ax4.bar(emotion_scores.keys(), emotion_scores.values())
            ax4.set_title("æ„Ÿæƒ…ã‚¹ã‚³ã‚¢åˆ†å¸ƒ")
            ax4.set_ylabel("ã‚¹ã‚³ã‚¢")
            
            plt.tight_layout()
            
            chart_path = os.path.join(self.output_dir, "copy_dna_analysis_chart.png")
            plt.savefig(chart_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"ğŸ“ˆ ã‚°ãƒ©ãƒ•ä¿å­˜: {chart_path}")
            
        except Exception as e:
            print(f"âš ï¸  ã‚°ãƒ©ãƒ•ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def print_summary(self, result: Dict[str, Any]) -> None:
        """çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        if not result.get("success"):
            print(f"\nâŒ å®Ÿè¡Œå¤±æ•—: {result.get('framework_info', {}).get('error', 'Unknown error')}")
            return
        
        print(f"\nğŸ“‹ {self.framework_name} å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼")
        print("=" * 50)
        
        # åŸºæœ¬æƒ…å ±
        print(f"ğŸ¯ åˆ†æå¯¾è±¡: {result['input_parameters']['brand_name']}")
        print(f"ğŸ“ ã‚³ãƒ”ãƒ¼æ•°: {result['input_parameters']['copy_count']}ä»¶")
        print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {result['framework_info']['execution_time']:.2f}ç§’")
        
        # DNAãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
        dna = result["dna_patterns"]
        print(f"\nğŸ§¬ DNAãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«: {dna['integrated_dna_profile']}")
        
        # èªå½™ç‰¹å¾´
        vocab = result["vocabulary_analysis"]
        print(f"\nğŸ“š èªå½™ç‰¹å¾´:")
        print(f"  èªå½™æ•°: {vocab['vocabulary_size']}èª")
        print(f"  èªå½™è±Šå¯Œåº¦: {vocab['vocabulary_richness']:.2f}")
        print(f"  é »å‡ºèª: {', '.join(list(vocab['top_words'].keys())[:5])}")
        
        # ãƒªã‚ºãƒ ç‰¹å¾´
        rhythm = result["rhythm_analysis"]
        print(f"\nğŸµ ãƒªã‚ºãƒ ç‰¹å¾´:")
        print(f"  å¹³å‡æ–‡é•·: {rhythm['avg_sentence_length']:.1f}æ–‡å­—")
        print(f"  æ„Ÿå˜†ç¬¦æ¯”ç‡: {rhythm['exclamation_ratio']:.2f}")
        dominant_rhythm = max(rhythm['rhythm_types'].items(), key=lambda x: x[1])[0] if rhythm['rhythm_types'] else "æ¨™æº–"
        print(f"  ä¸»è¦ãƒªã‚ºãƒ : {dominant_rhythm}")
        
        # æ„Ÿæƒ…ç‰¹å¾´
        emotion = result["emotion_analysis"]
        print(f"\nğŸ˜Š æ„Ÿæƒ…ç‰¹å¾´:")
        print(f"  ä¸»è¦æ„Ÿæƒ…: {emotion['dominant_emotion']}")
        print(f"  æ„Ÿæƒ…å¼·åº¦: {emotion['emotional_intensity']:.2f}")
        
        # æ´å¯Ÿ
        print(f"\nğŸ’¡ ä¸»è¦æ´å¯Ÿ:")
        for insight in result["insights"]:
            print(f"  â€¢ {insight}")
        
        print(f"\nğŸ“ è©³ç´°çµæœã¯ {self.output_dir} ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
SAMPLE_DATA = {
    "copy_samples": [
        "æ–°ã—ã„æœãŒå§‹ã¾ã‚‹ã€‚ä¸€æ¯ã®ã‚³ãƒ¼ãƒ’ãƒ¼ã‹ã‚‰ã€‚",
        "ç¾å‘³ã—ã•ã«ã€ã“ã ã‚ã‚Šç¶šã‘ã¦50å¹´ã€‚",
        "ã‚ãªãŸã®æ¯æ—¥ã‚’ã€ã‚‚ã£ã¨ç´ æ™´ã‚‰ã—ãï¼",
        "ä»Šã™ãä½“é¨“ã—ã¦ãã ã•ã„ã€‚ã“ã®æ„Ÿå‹•ã‚’ã€‚",
        "å¥åº·ã§ç¾ã—ã„è‚Œã¸ã€‚è‡ªç„¶ã®åŠ›ã§ã€‚",
        "é™å®šç™ºå£²ï¼ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ãªå‘³ã‚ã„ã‚’ã€‚",
        "å®¶æ—ã¿ã‚“ãªã§æ¥½ã—ã‚ã‚‹ã€å®‰å¿ƒã®å“è³ªã€‚",
        "é©æ–°çš„ãªæŠ€è¡“ãŒã€æœªæ¥ã‚’å¤‰ãˆã‚‹ã€‚"
    ],
    "brand_name": "ã‚µãƒ³ãƒ—ãƒ«ãƒ–ãƒ©ãƒ³ãƒ‰"
}

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="Copy DNA Audit åˆ†æãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯")
    parser.add_argument("--brand", default=SAMPLE_DATA["brand_name"], help="ãƒ–ãƒ©ãƒ³ãƒ‰å")
    parser.add_argument("--file", help="ã‚³ãƒ”ãƒ¼ãƒªã‚¹ãƒˆãŒå«ã¾ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument("--config", help="è¨­å®šJSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")
    
    args = parser.parse_args()
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    if args.config and os.path.exists(args.config):
        with open(args.config, 'r', encoding='utf-8') as f:
            config = json.load(f)
        copy_samples = config.get("copy_samples", SAMPLE_DATA["copy_samples"])
        brand_name = config.get("brand_name", args.brand)
    elif args.file and os.path.exists(args.file):
        with open(args.file, 'r', encoding='utf-8') as f:
            copy_samples = [line.strip() for line in f.readlines() if line.strip()]
        brand_name = args.brand
    else:
        copy_samples = SAMPLE_DATA["copy_samples"]
        brand_name = args.brand
        print("ğŸ”§ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦å®Ÿè¡Œã—ã¾ã™")
    
    # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å®Ÿè¡Œ
    framework = CopyDNAFramework()
    result = await framework.collect_data(copy_samples, brand_name)
    
    # çµæœä¿å­˜ãƒ»è¡¨ç¤º
    framework.save_results(result)
    framework.print_summary(result)

if __name__ == "__main__":
    asyncio.run(main()) 